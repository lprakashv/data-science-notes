

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Advanced NLP &#8212; Lalit&#39;s AI / ML Notebook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'advanced_nlp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Neural Networks" href="deep_learning_apps.html" />
    <link rel="prev" title="Natural Language Processing (NLP)" href="nlp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Lalit's AI / ML Notebook - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Lalit's AI / ML Notebook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    <no title>
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Numpy</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="numpy_intro.html">Intro to Numpy for Data Science</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recommendation System</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="recommendation.html">Recommendation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">NLP</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Natural Language Processing (NLP)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Advanced NLP</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="deep_learning_apps.html">Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/lprakashv/data-science-notes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/lprakashv/data-science-notes/issues/new?title=Issue%20on%20page%20%2Fadvanced_nlp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/advanced_nlp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Advanced NLP</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-pipeline">Basic pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#read-and-clean">Read and clean:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#randomforestclassifier">RandomForestClassifier</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">word2vec</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uses-cosine-similarity">uses “Cosine similarity”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explore-pre-trained-embeddings">Explore Pre-trained Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-our-own-model">Train our own model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prep-word-vectors">Prep word vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#doc2vec">doc2vec</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-our-own-model-for-doc2vec">Train our own model (for doc2vec)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-about-pre-trained-document-vectors">What About Pre-trained Document Vectors?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-based-vector-model">Deep Learning - based Vector model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-basic-rnn">Implement basic RNN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#read-clean-and-split-the-data">Read, clean and split the data:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prep-data-for-modelling">Prep data for modelling:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#build-model">Build model:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-the-nlp-techniques">Comparing the NLP Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-data-for-modelling">Preparing the data for modelling:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-model-on-tf-idf-vectors">Build Model on TF-IDF Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-tf-idf-vectors">Create TF-IDF Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-random-forest-on-top-of-the-vectors">Fit Random Forest on top of the Vectors</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-model-on-word2vec-vectors">Build Model on word2vec Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-word2vec-vectors">Create word2vec Vectors:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-randomforestclassifier-on-top-of-word2vec-vectors">Fit RandomForestClassifier on top of word2vec Vectors:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-model-on-doc2vec-vectors">Build Model on doc2vec Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-doc2vec-vectors">Creating doc2vec Vectors:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-randomforestclassifier-on-top-of-document-vectors">Fit RandomForestClassifier on top of Document Vectors:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-basic-rnn">Build basic RNN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prep-the-data-for-rnn">Prep the data for RNN:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#build-and-evaluate-rnn">Build and Evaluate RNN:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-the-comparison">Summary of the Comparison</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf">TF-IDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">word2vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">doc2vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Recurrent Neural Network (RNN)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#takeway">Takeway</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="advanced-nlp">
<h1>Advanced NLP<a class="headerlink" href="#advanced-nlp" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>word2vec</p></li>
<li><p>doc2vec</p></li>
<li><p>RNN</p></li>
</ul>
<section id="basic-pipeline">
<h2>Basic pipeline<a class="headerlink" href="#basic-pipeline" title="Permalink to this heading">#</a></h2>
<section id="read-and-clean">
<h3>Read and clean:<a class="headerlink" href="#read-and-clean" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read in, clean, and vectorize data</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./spam.csv&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;latin-1&#39;</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="n">messages</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Unnamed: 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 4&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">messages</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">])</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;\W+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">text</span>

<span class="n">tfidf_vect</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">X_tfidf</span> <span class="o">=</span> <span class="n">tfidf_vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">messages</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>

<span class="n">X_features</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_tfidf</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="n">X_features</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>9385</th>
      <th>9386</th>
      <th>9387</th>
      <th>9388</th>
      <th>9389</th>
      <th>9390</th>
      <th>9391</th>
      <th>9392</th>
      <th>9393</th>
      <th>9394</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 9395 columns</p>
</div>
</section>
<section id="randomforestclassifier">
<h3>RandomForestClassifier<a class="headerlink" href="#randomforestclassifier" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import Random Forest for classification from sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the methods that will be needed to evaluate a basic model</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split data into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_features</span><span class="p">,</span>
                                                    <span class="n">messages</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span>
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit a basic Random Forest model</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make predictions on the test set using the fit model</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evalute model predictions using precision and recall</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s1">&#39;spam&#39;</span><span class="p">)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s1">&#39;spam&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Precision: </span><span class="si">{}</span><span class="s1"> / Recall: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Precision: 1.0 / Recall: 0.805
</pre></div>
</div>
</section>
</section>
<section id="word2vec">
<h2>word2vec<a class="headerlink" href="#word2vec" title="Permalink to this heading">#</a></h2>
<p>word2vec is a shallow, 2-layer neural network that accepts a text corpus as an input, and it returns a set of vectors (also known as embeddings); each vector is a numeric representatiopn of a given word.</p>
<p><em>“You shall know the word by the company it keeps.”</em></p>
<section id="uses-cosine-similarity">
<h3>uses “Cosine similarity”<a class="headerlink" href="#uses-cosine-similarity" title="Permalink to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(\vec{Queen} = \vec{King} - \vec{Man} + \vec{Woman}\)</span></p>
</section>
<section id="explore-pre-trained-embeddings">
<h3>Explore Pre-trained Embeddings<a class="headerlink" href="#explore-pre-trained-embeddings" title="Permalink to this heading">#</a></h3>
<p>Some other options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">glove-twitter-{25/50/100/200}</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">glove-wiki-gigaword-{50/200/300}</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">word2vec-google-news-300</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">word2vec-ruscorpora-news-300</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load pretrained word vectors using gensim</span>
<span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>

<span class="n">wiki_embeddings</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;glove-wiki-gigaword-100&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Explore the word vector for &quot;king&quot;</span>
<span class="n">wiki_embeddings</span><span class="p">[</span><span class="s1">&#39;king&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,
       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,
       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,
       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,
        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,
        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,
       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,
        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,
        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,
       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,
        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,
       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,
        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,
       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,
       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,
        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,
       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find the words most similar to king based on the trained word vectors</span>
<span class="n">wiki_embeddings</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;king&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[(&#39;prince&#39;, 0.7682329416275024),
 (&#39;queen&#39;, 0.7507690787315369),
 (&#39;son&#39;, 0.7020887732505798),
 (&#39;brother&#39;, 0.6985775232315063),
 (&#39;monarch&#39;, 0.6977890729904175),
 (&#39;throne&#39;, 0.691999077796936),
 (&#39;kingdom&#39;, 0.6811410188674927),
 (&#39;father&#39;, 0.680202841758728),
 (&#39;emperor&#39;, 0.6712858080863953),
 (&#39;ii&#39;, 0.6676074266433716)]
</pre></div>
</div>
</section>
<section id="train-our-own-model">
<h3>Train our own model<a class="headerlink" href="#train-our-own-model" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read in the data and clean up column names</span>
<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_colwidth&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./spam.csv&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;latin-1&#39;</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="n">messages</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Unnamed: 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 4&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">messages</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">messages</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives around here though</td>
    </tr>
  </tbody>
</table>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clean data using the built in cleaner in gensim</span>
<span class="n">messages</span><span class="p">[</span><span class="s1">&#39;text_clean&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">gensim</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">simple_preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>text_clean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>
      <td>[go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
      <td>[ok, lar, joking, wif, oni]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>
      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
      <td>[dun, say, so, early, hor, already, then, say]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives around here though</td>
      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>
    </tr>
  </tbody>
</table>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split data into train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">messages</span><span class="p">[</span><span class="s1">&#39;text_clean&#39;</span><span class="p">],</span>
                                                    <span class="n">messages</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the word2vec model</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span>
                                   <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                   <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                   <span class="n">min_count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Explore the word vector for &quot;king&quot; base on our trained model</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;king&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>array([ 8.22794437e-02,  7.15377703e-02,  1.13073792e-02, -6.92935893e-04,
       -5.78757329e-03, -2.24397099e-03,  3.08556780e-02,  1.28532007e-01,
       -4.49366197e-02,  1.40099856e-03,  9.39837322e-02,  9.83330309e-02,
        2.96608564e-02,  1.88523494e-02,  7.69960880e-02, -2.38459632e-02,
       -2.20810138e-02, -4.47637960e-02, -3.52773368e-02, -3.94348390e-02,
        2.40708720e-02,  3.34583744e-02,  9.19674989e-03, -6.58849776e-02,
        3.25308107e-02,  7.00966362e-03, -1.93505250e-02, -7.13438392e-02,
       -5.31851910e-02, -1.93836279e-02, -5.48255108e-02, -4.52534631e-02,
       -1.12883314e-01, -4.39974666e-02, -3.12781222e-02,  3.94870266e-02,
        2.48225518e-02,  1.02029257e-01,  1.68355368e-02, -2.96352208e-02,
       -4.72212359e-02, -1.04903663e-02,  2.51805969e-02,  4.88484688e-02,
        3.09625524e-03,  4.78431545e-02,  1.08772004e-02,  3.26122977e-02,
       -8.82226601e-02, -8.90500620e-02, -7.91083947e-02,  7.29472116e-02,
       -2.14259792e-02,  1.16243578e-01,  5.48660718e-02,  8.18767399e-02,
        3.06534115e-04, -1.09556250e-01,  8.95170122e-02, -7.96145499e-02,
        9.35777426e-02,  3.22407782e-02, -3.69137502e-03, -9.48343202e-02,
       -1.49107471e-01,  2.22490802e-02, -2.27584466e-02,  4.20865230e-02,
        7.94619173e-02,  1.12201291e-04, -8.41916446e-03,  1.47700962e-02,
        2.55049989e-02, -2.59182503e-04, -6.56038299e-02, -3.70079242e-02,
       -6.59312084e-02, -2.83849202e-02, -1.01926044e-01, -5.59506528e-02,
        6.67202100e-02, -6.29046559e-02,  6.54808506e-02, -8.57574195e-02,
        9.10010710e-02,  2.61553973e-02,  1.59907732e-02, -2.33701877e-02,
        6.09138459e-02,  4.69054002e-03, -4.47405912e-02, -8.00395831e-02,
        5.79755083e-02,  4.76236120e-02, -8.65149274e-02,  3.24716941e-02,
       -1.27660129e-02, -7.21691623e-02,  1.85737591e-02, -1.11927148e-02],
      dtype=float32)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find the most similar words to &quot;king&quot; based on word vectors from our trained model</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;king&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[(&#39;sex&#39;, 0.9987980127334595),
 (&#39;then&#39;, 0.9987930059432983),
 (&#39;same&#39;, 0.9987838268280029),
 (&#39;sms&#39;, 0.9987767934799194),
 (&#39;stuff&#39;, 0.9987733960151672),
 (&#39;of&#39;, 0.9987660050392151),
 (&#39;but&#39;, 0.9987635612487793),
 (&#39;even&#39;, 0.9987606406211853),
 (&#39;see&#39;, 0.9987546801567078),
 (&#39;real&#39;, 0.9987517595291138)]
</pre></div>
</div>
</section>
<section id="prep-word-vectors">
<h3>Prep word vectors<a class="headerlink" href="#prep-word-vectors" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate a list of words the word2vec model learned word vectors for</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[&#39;you&#39;,
 &#39;to&#39;,
 &#39;the&#39;,
 &#39;and&#39;,
 &#39;in&#39;,
 &#39;is&#39;,
 &#39;my&#39;,
 &#39;me&#39;,
 &#39;it&#39;,
 &#39;for&#39;,
 &#39;your&#39;,
 &#39;of&#39;,
 &#39;call&#39;,
 &#39;have&#39;,
 &#39;that&#39;,
 &#39;on&#39;,
 &#39;are&#39;,
 &#39;now&#39;,
 &#39;not&#39;,
 &#39;can&#39;,
 &#39;but&#39;,
 &#39;so&#39;,
 &#39;or&#39;,
 &#39;do&#39;,
 &#39;we&#39;,
 &#39;at&#39;,
 &#39;with&#39;,
 &#39;be&#39;,
 &#39;get&#39;,
 &#39;if&#39;,
 &#39;will&#39;,
 &#39;ur&#39;,
 &#39;just&#39;,
 &#39;no&#39;,
 &#39;this&#39;,
 &#39;up&#39;,
 &#39;gt&#39;,
 &#39;lt&#39;,
 &#39;how&#39;,
 &#39;free&#39;,
 &#39;when&#39;,
 &#39;from&#39;,
 &#39;what&#39;,
 &#39;ok&#39;,
 &#39;all&#39;,
 &#39;go&#39;,
 &#39;know&#39;,
 &#39;out&#39;,
 &#39;ll&#39;,
 &#39;like&#39;,
 &#39;good&#39;,
 &#39;day&#39;,
 &#39;am&#39;,
 &#39;got&#39;,
 &#39;was&#39;,
 &#39;come&#39;,
 &#39;he&#39;,
 &#39;its&#39;,
 &#39;there&#39;,
 &#39;then&#39;,
 &#39;only&#39;,
 &#39;time&#39;,
 &#39;love&#39;,
 &#39;text&#39;,
 &#39;want&#39;,
 &#39;send&#39;,
 &#39;as&#39;,
 &#39;txt&#39;,
 &#39;one&#39;,
 &#39;see&#39;,
 &#39;by&#39;,
 &#39;need&#39;,
 &#39;about&#39;,
 &#39;lor&#39;,
 &#39;today&#39;,
 &#39;going&#39;,
 &#39;home&#39;,
 &#39;she&#39;,
 &#39;stop&#39;,
 &#39;still&#39;,
 &#39;sorry&#39;,
 &#39;da&#39;,
 &#39;don&#39;,
 &#39;our&#39;,
 &#39;reply&#39;,
 &#39;back&#39;,
 &#39;please&#39;,
 &#39;tell&#39;,
 &#39;dont&#39;,
 &#39;new&#39;,
 &#39;pls&#39;,
 &#39;later&#39;,
 &#39;did&#39;,
 &#39;they&#39;,
 &#39;mobile&#39;,
 &#39;her&#39;,
 &#39;hi&#39;,
 &#39;phone&#39;,
 &#39;think&#39;,
 &#39;week&#39;,
 &#39;take&#39;,
 &#39;any&#39;,
 &#39;well&#39;,
 &#39;been&#39;,
 &#39;dear&#39;,
 &#39;some&#39;,
 &#39;where&#39;,
 &#39;re&#39;,
 &#39;him&#39;,
 &#39;here&#39;,
 &#39;has&#39;,
 &#39;who&#39;,
 &#39;msg&#39;,
 &#39;happy&#39;,
 &#39;an&#39;,
 &#39;ì_&#39;,
 &#39;oh&#39;,
 &#39;hope&#39;,
 &#39;too&#39;,
 &#39;more&#39;,
 &#39;much&#39;,
 &#39;great&#39;,
 &#39;should&#39;,
 &#39;night&#39;,
 &#39;make&#39;,
 &#39;way&#39;,
 &#39;message&#39;,
 &#39;claim&#39;,
 &#39;hey&#39;,
 &#39;wat&#39;,
 &#39;www&#39;,
 &#39;had&#39;,
 &#39;work&#39;,
 &#39;yes&#39;,
 &#39;number&#39;,
 &#39;won&#39;,
 &#39;said&#39;,
 &#39;ve&#39;,
 &#39;right&#39;,
 &#39;tomorrow&#39;,
 &#39;prize&#39;,
 &#39;after&#39;,
 &#39;give&#39;,
 &#39;amp&#39;,
 &#39;doing&#39;,
 &#39;say&#39;,
 &#39;yeah&#39;,
 &#39;really&#39;,
 &#39;meet&#39;,
 &#39;life&#39;,
 &#39;win&#39;,
 &#39;ask&#39;,
 &#39;why&#39;,
 &#39;them&#39;,
 &#39;cash&#39;,
 &#39;com&#39;,
 &#39;thanks&#39;,
 &#39;find&#39;,
 &#39;very&#39;,
 &#39;uk&#39;,
 &#39;im&#39;,
 &#39;miss&#39;,
 &#39;already&#39;,
 &#39;would&#39;,
 &#39;babe&#39;,
 &#39;lol&#39;,
 &#39;buy&#39;,
 &#39;let&#39;,
 &#39;morning&#39;,
 &#39;cos&#39;,
 &#39;keep&#39;,
 &#39;every&#39;,
 &#39;also&#39;,
 &#39;service&#39;,
 &#39;urgent&#39;,
 &#39;min&#39;,
 &#39;contact&#39;,
 &#39;again&#39;,
 &#39;anything&#39;,
 &#39;sent&#39;,
 &#39;sure&#39;,
 &#39;pick&#39;,
 &#39;last&#39;,
 &#39;nokia&#39;,
 &#39;over&#39;,
 &#39;first&#39;,
 &#39;which&#39;,
 &#39;always&#39;,
 &#39;nice&#39;,
 &#39;us&#39;,
 &#39;help&#39;,
 &#39;were&#39;,
 &#39;went&#39;,
 &#39;cant&#39;,
 &#39;something&#39;,
 &#39;wait&#39;,
 &#39;money&#39;,
 &#39;care&#39;,
 &#39;per&#39;,
 &#39;next&#39;,
 &#39;around&#39;,
 &#39;box&#39;,
 &#39;thing&#39;,
 &#39;off&#39;,
 &#39;gonna&#39;,
 &#39;sleep&#39;,
 &#39;place&#39;,
 &#39;his&#39;,
 &#39;could&#39;,
 &#39;pm&#39;,
 &#39;chat&#39;,
 &#39;down&#39;,
 &#39;may&#39;,
 &#39;customer&#39;,
 &#39;feel&#39;,
 &#39;many&#39;,
 &#39;tone&#39;,
 &#39;soon&#39;,
 &#39;even&#39;,
 &#39;sms&#39;,
 &#39;mins&#39;,
 &#39;someone&#39;,
 &#39;friends&#39;,
 &#39;name&#39;,
 &#39;tonight&#39;,
 &#39;late&#39;,
 &#39;gud&#39;,
 &#39;hello&#39;,
 &#39;co&#39;,
 &#39;ya&#39;,
 &#39;st&#39;,
 &#39;people&#39;,
 &#39;wish&#39;,
 &#39;friend&#39;,
 &#39;leave&#39;,
 &#39;done&#39;,
 &#39;special&#39;,
 &#39;waiting&#39;,
 &#39;before&#39;,
 &#39;guaranteed&#39;,
 &#39;same&#39;,
 &#39;other&#39;,
 &#39;told&#39;,
 &#39;ìï&#39;,
 &#39;thought&#39;,
 &#39;yet&#39;,
 &#39;use&#39;,
 &#39;haha&#39;,
 &#39;dun&#39;,
 &#39;live&#39;,
 &#39;wan&#39;,
 &#39;god&#39;,
 &#39;heart&#39;,
 &#39;cool&#39;,
 &#39;getting&#39;,
 &#39;year&#39;,
 &#39;days&#39;,
 &#39;th&#39;,
 &#39;mind&#39;,
 &#39;best&#39;,
 &#39;holiday&#39;,
 &#39;ppm&#39;,
 &#39;draw&#39;,
 &#39;meeting&#39;,
 &#39;try&#39;,
 &#39;lunch&#39;,
 &#39;coming&#39;,
 &#39;class&#39;,
 &#39;man&#39;,
 &#39;talk&#39;,
 &#39;smile&#39;,
 &#39;stuff&#39;,
 &#39;end&#39;,
 &#39;things&#39;,
 &#39;wk&#39;,
 &#39;cs&#39;,
 &#39;bit&#39;,
 &#39;fine&#39;,
 &#39;job&#39;,
 &#39;thk&#39;,
 &#39;line&#39;,
 &#39;being&#39;,
 &#39;pobox&#39;,
 &#39;sir&#39;,
 &#39;long&#39;,
 &#39;trying&#39;,
 &#39;play&#39;,
 &#39;enjoy&#39;,
 &#39;having&#39;,
 &#39;car&#39;,
 &#39;thats&#39;,
 &#39;ready&#39;,
 &#39;better&#39;,
 &#39;didn&#39;,
 &#39;wanna&#39;,
 &#39;finish&#39;,
 &#39;ill&#39;,
 &#39;few&#39;,
 &#39;because&#39;,
 &#39;yup&#39;,
 &#39;house&#39;,
 &#39;half&#39;,
 &#39;bt&#39;,
 &#39;month&#39;,
 &#39;world&#39;,
 &#39;never&#39;,
 &#39;dat&#39;,
 &#39;big&#39;,
 &#39;jus&#39;,
 &#39;receive&#39;,
 &#39;word&#39;,
 &#39;account&#39;,
 &#39;person&#39;,
 &#39;lot&#39;,
 &#39;guess&#39;,
 &#39;chance&#39;,
 &#39;ah&#39;,
 &#39;room&#39;,
 &#39;than&#39;,
 &#39;weekend&#39;,
 &#39;check&#39;,
 &#39;sweet&#39;,
 &#39;into&#39;,
 &#39;pa&#39;,
 &#39;birthday&#39;,
 &#39;po&#39;,
 &#39;hrs&#39;,
 &#39;watching&#39;,
 &#39;shows&#39;,
 &#39;shit&#39;,
 &#39;might&#39;,
 &#39;boy&#39;,
 &#39;missing&#39;,
 &#39;landline&#39;,
 &#39;yo&#39;,
 &#39;start&#39;,
 &#39;early&#39;,
 &#39;real&#39;,
 &#39;lar&#39;,
 &#39;ever&#39;,
 &#39;video&#39;,
 &#39;minutes&#39;,
 &#39;thanx&#39;,
 &#39;another&#39;,
 &#39;once&#39;,
 &#39;enough&#39;,
 &#39;princess&#39;,
 &#39;camera&#39;,
 &#39;part&#39;,
 &#39;xxx&#39;,
 &#39;dunno&#39;,
 &#39;offer&#39;,
 &#39;girl&#39;,
 &#39;between&#39;,
 &#39;quite&#39;,
 &#39;awarded&#39;,
 &#39;sat&#39;,
 &#39;pay&#39;,
 &#39;cost&#39;,
 &#39;den&#39;,
 &#39;two&#39;,
 &#39;eat&#39;,
 &#39;called&#39;,
 &#39;didnt&#39;,
 &#39;face&#39;,
 &#39;ringtone&#39;,
 &#39;wife&#39;,
 &#39;evening&#39;,
 &#39;luv&#39;,
 &#39;aight&#39;,
 &#39;liao&#39;,
 &#39;nothing&#39;,
 &#39;remember&#39;,
 &#39;tv&#39;,
 &#39;nite&#39;,
 &#39;nd&#39;,
 &#39;rate&#39;,
 &#39;speak&#39;,
 &#39;bed&#39;,
 &#39;code&#39;,
 &#39;easy&#39;,
 &#39;tones&#39;,
 &#39;hear&#39;,
 &#39;actually&#39;,
 &#39;does&#39;,
 &#39;problem&#39;,
 &#39;dis&#39;,
 &#39;probably&#39;,
 &#39;everything&#39;,
 &#39;award&#39;,
 &#39;guys&#39;,
 &#39;shall&#39;,
 &#39;look&#39;,
 &#39;join&#39;,
 &#39;wont&#39;,
 &#39;okay&#39;,
 &#39;collection&#39;,
 &#39;age&#39;,
 &#39;goes&#39;,
 &#39;working&#39;,
 &#39;put&#39;,
 &#39;afternoon&#39;,
 &#39;dinner&#39;,
 &#39;xmas&#39;,
 &#39;latest&#39;,
 &#39;apply&#39;,
 &#39;wot&#39;,
 &#39;baby&#39;,
 &#39;maybe&#39;,
 &#39;those&#39;,
 &#39;messages&#39;,
 &#39;plan&#39;,
 &#39;calls&#39;,
 &#39;tmr&#39;,
 &#39;plus&#39;,
 &#39;gift&#39;,
 &#39;show&#39;,
 &#39;little&#39;,
 &#39;wake&#39;,
 &#39;until&#39;,
 &#39;came&#39;,
 &#39;made&#39;,
 &#39;most&#39;,
 &#39;orange&#39;,
 &#39;details&#39;,
 &#39;times&#39;,
 &#39;forgot&#39;,
 &#39;years&#39;,
 &#39;shopping&#39;,
 &#39;watch&#39;,
 &#39;sexy&#39;,
 &#39;kiss&#39;,
 &#39;thank&#39;,
 &#39;collect&#39;,
 &#39;asked&#39;,
 &#39;selected&#39;,
 &#39;mail&#39;,
 &#39;leh&#39;,
 &#39;book&#39;,
 &#39;hour&#39;,
 &#39;decimal&#39;,
 &#39;bad&#39;,
 &#39;reach&#39;,
 &#39;hot&#39;,
 &#39;de&#39;,
 &#39;town&#39;,
 &#39;missed&#39;,
 &#39;school&#39;,
 &#39;club&#39;,
 &#39;fun&#39;,
 &#39;havent&#39;,
 &#39;bus&#39;,
 &#39;mob&#39;,
 &#39;texts&#39;,
 &#39;oso&#39;,
 &#39;while&#39;,
 &#39;pain&#39;,
 &#39;though&#39;,
 &#39;hair&#39;,
 &#39;valid&#39;,
 &#39;online&#39;,
 &#39;network&#39;,
 &#39;sch&#39;,
 &#39;important&#39;,
 &#39;must&#39;,
 &#39;lei&#39;,
 &#39;hav&#39;,
 &#39;stay&#39;,
 &#39;date&#39;,
 &#39;words&#39;,
 &#39;sleeping&#39;,
 &#39;left&#39;,
 &#39;yesterday&#39;,
 &#39;looking&#39;,
 &#39;office&#39;,
 &#39;food&#39;,
 &#39;making&#39;,
 &#39;lose&#39;,
 &#39;entry&#39;,
 &#39;away&#39;,
 &#39;able&#39;,
 &#39;juz&#39;,
 &#39;change&#39;,
 &#39;else&#39;,
 &#39;yours&#39;,
 &#39;alright&#39;,
 &#39;music&#39;,
 &#39;abt&#39;,
 &#39;weekly&#39;,
 &#39;id&#39;,
 &#39;took&#39;,
 &#39;game&#39;,
 &#39;true&#39;,
 &#39;order&#39;,
 &#39;wen&#39;,
 &#39;fuck&#39;,
 &#39;means&#39;,
 &#39;says&#39;,
 &#39;haf&#39;,
 &#39;ard&#39;,
 &#39;national&#39;,
 &#39;xx&#39;,
 &#39;hurt&#39;,
 &#39;old&#39;,
 &#39;worry&#39;,
 &#39;tried&#39;,
 &#39;gr&#39;,
 &#39;friendship&#39;,
 &#39;either&#39;,
 &#39;await&#39;,
 &#39;sae&#39;,
 &#39;tot&#39;,
 &#39;full&#39;,
 &#39;hours&#39;,
 &#39;family&#39;,
 &#39;address&#39;,
 &#39;happen&#39;,
 &#39;anyway&#39;,
 &#39;makes&#39;,
 &#39;sad&#39;,
 &#39;games&#39;,
 &#39;yourself&#39;,
 &#39;goin&#39;,
 &#39;question&#39;,
 &#39;story&#39;,
 &#39;mean&#39;,
 &#39;dad&#39;,
 &#39;price&#39;,
 &#39;delivery&#39;,
 &#39;okie&#39;,
 &#39;wif&#39;,
 &#39;dude&#39;,
 &#39;these&#39;,
 &#39;since&#39;,
 &#39;without&#39;,
 &#39;trip&#39;,
 &#39;plz&#39;,
 &#39;haven&#39;,
 &#39;net&#39;,
 &#39;feeling&#39;,
 &#39;comes&#39;,
 &#39;poly&#39;,
 &#39;email&#39;,
 &#39;parents&#39;,
 &#39;finished&#39;,
 &#39;saw&#39;,
 &#39;head&#39;,
 &#39;run&#39;,
 &#39;eve&#39;,
 &#39;smiling&#39;,
 &#39;congrats&#39;,
 &#39;calling&#39;,
 &#39;wid&#39;,
 &#39;drive&#39;,
 &#39;points&#39;,
 &#39;news&#39;,
 &#39;brother&#39;,
 &#39;mine&#39;,
 &#39;minute&#39;,
 &#39;pic&#39;,
 &#39;pics&#39;,
 &#39;pub&#39;,
 &#39;charge&#39;,
 &#39;wanted&#39;,
 &#39;top&#39;,
 &#39;dating&#39;,
 &#39;till&#39;,
 &#39;optout&#39;,
 &#39;rite&#39;,
 &#39;http&#39;,
 &#39;sun&#39;,
 &#39;todays&#39;,
 &#39;bring&#39;,
 &#39;attempt&#39;,
 &#39;answer&#39;,
 &#39;taking&#39;,
 &#39;leaving&#39;,
 &#39;info&#39;,
 &#39;close&#39;,
 &#39;shop&#39;,
 &#39;movie&#39;,
 &#39;huh&#39;,
 &#39;update&#39;,
 &#39;test&#39;,
 &#39;dreams&#39;,
 &#39;bonus&#39;,
 &#39;wil&#39;,
 &#39;guy&#39;,
 &#39;bored&#39;,
 &#39;private&#39;,
 &#39;company&#39;,
 &#39;found&#39;,
 &#39;driving&#39;,
 &#39;lets&#39;,
 &#39;both&#39;,
 &#39;nt&#39;,
 &#39;together&#39;,
 &#39;wants&#39;,
 &#39;wrong&#39;,
 &#39;crazy&#39;,
 &#39;needs&#39;,
 &#39;choose&#39;,
 &#39;mom&#39;,
 &#39;boytoy&#39;,
 &#39;thinking&#39;,
 &#39;aft&#39;,
 &#39;touch&#39;,
 &#39;lots&#39;,
 &#39;search&#39;,
 &#39;chennai&#39;,
 &#39;tomo&#39;,
 &#39;til&#39;,
 &#39;ring&#39;,
 &#39;fast&#39;,
 &#39;noe&#39;,
 &#39;awesome&#39;,
 &#39;college&#39;,
 &#39;busy&#39;,
 &#39;available&#39;,
 &#39;colour&#39;,
 &#39;everyone&#39;,
 &#39;set&#39;,
 &#39;smoke&#39;,
 &#39;neva&#39;,
 &#39;gbp&#39;,
 &#39;loving&#39;,
 &#39;vouchers&#39;,
 &#39;takes&#39;,
 &#39;sunday&#39;,
 &#39;beautiful&#39;,
 &#39;forget&#39;,
 &#39;gd&#39;,
 &#39;double&#39;,
 &#39;drink&#39;,
 &#39;decided&#39;,
 &#39;sister&#39;,
 &#39;mrng&#39;,
 &#39;msgs&#39;,
 &#39;yr&#39;,
 &#39;unsubscribe&#39;,
 &#39;anytime&#39;,
 &#39;break&#39;,
 &#39;coz&#39;,
 &#39;mths&#39;,
 &#39;lovely&#39;,
 &#39;happened&#39;,
 &#39;land&#39;,
 &#39;blue&#39;,
 &#39;open&#39;,
 &#39;statement&#39;,
 &#39;expires&#39;,
 &#39;visit&#39;,
 &#39;lesson&#39;,
 &#39;mum&#39;,
 &#39;smth&#39;,
 &#39;card&#39;,
 &#39;anyone&#39;,
 &#39;eh&#39;,
 &#39;chikku&#39;,
 &#39;walk&#39;,
 &#39;pounds&#39;,
 &#39;started&#39;,
 &#39;each&#39;,
 &#39;carlos&#39;,
 &#39;christmas&#39;,
 &#39;sk&#39;,
 &#39;alone&#39;,
 &#39;goodmorning&#39;,
 &#39;finally&#39;,
 &#39;content&#39;,
 &#39;sounds&#39;,
 &#39;frnds&#39;,
 &#39;tc&#39;,
 &#39;treat&#39;,
 &#39;hand&#39;,
 &#39;oredi&#39;,
 &#39;kind&#39;,
 &#39;û_&#39;,
 &#39;pretty&#39;,
 &#39;ltd&#39;,
 &#39;far&#39;,
 &#39;party&#39;,
 &#39;sex&#39;,
 &#39;darlin&#39;,
 &#39;drop&#39;,
 &#39;hmm&#39;,
 &#39;seeing&#39;,
 &#39;hows&#39;,
 &#39;ten&#39;,
 &#39;identifier&#39;,
 &#39;light&#39;,
 &#39;congratulations&#39;,
 &#39;type&#39;,
 &#39;unlimited&#39;,
 &#39;grins&#39;,
 &#39;ha&#39;,
 &#39;saturday&#39;,
 &#39;outside&#39;,
 &#39;cause&#39;,
 &#39;download&#39;,
 &#39;whats&#39;,
 &#39;least&#39;,
 &#39;surprise&#39;,
 &#39;worth&#39;,
 &#39;happiness&#39;,
 &#39;opt&#39;,
 &#39;services&#39;,
 &#39;song&#39;,
 &#39;rs&#39;,
 &#39;sis&#39;,
 &#39;final&#39;,
 &#39;lucky&#39;,
 &#39;log&#39;,
 &#39;almost&#39;,
 &#39;felt&#39;,
 &#39;john&#39;,
 &#39;tho&#39;,
 &#39;hl&#39;,
 &#39;hit&#39;,
 &#39;saying&#39;,
 &#39;auction&#39;,
 &#39;credit&#39;,
 &#39;uncle&#39;,
 &#39;gas&#39;,
 &#39;winner&#39;,
 &#39;computer&#39;,
 &#39;hard&#39;,
 &#39;ni&#39;,
 &#39;prob&#39;,
 &#39;seen&#39;,
 &#39;case&#39;,
 &#39;believe&#39;,
 &#39;doesn&#39;,
 &#39;wonderful&#39;,
 &#39;fancy&#39;,
 &#39;welcome&#39;,
 &#39;row&#39;,
 &#39;player&#39;,
 &#39;quiz&#39;,
 &#39;fr&#39;,
 &#39;bout&#39;,
 &#39;father&#39;,
 &#39;freemsg&#39;,
 &#39;wq&#39;,
 &#39;simple&#39;,
 &#39;na&#39;,
 &#39;lost&#39;,
 &#39;sea&#39;,
 &#39;post&#39;,
 &#39;mah&#39;,
 &#39;snow&#39;,
 &#39;met&#39;,
 &#39;gn&#39;,
 &#39;exam&#39;,
 &#39;friday&#39;,
 &#39;mate&#39;,
 &#39;mobileupd&#39;,
 &#39;knw&#39;,
 &#39;numbers&#39;,
 &#39;cum&#39;,
 &#39;information&#39;,
 &#39;phones&#39;,
 &#39;side&#39;,
 &#39;joy&#39;,
 &#39;reason&#39;,
 &#39;used&#39;,
 &#39;weeks&#39;,
 &#39;camcorder&#39;,
 &#39;hmmm&#39;,
 &#39;operator&#39;,
 &#39;ago&#39;,
 &#39;plans&#39;,
 &#39;picking&#39;,
 &#39;area&#39;,
 &#39;gal&#39;,
 &#39;fucking&#39;,
 &#39;angry&#39;,
 &#39;earlier&#39;,
 &#39;bath&#39;,
 &#39;tel&#39;,
 &#39;girls&#39;,
 &#39;support&#39;,
 &#39;their&#39;,
 &#39;fone&#39;,
 &#39;un&#39;,
 &#39;months&#39;,
 &#39;voucher&#39;,
 &#39;police&#39;,
 &#39;telling&#39;,
 &#39;frnd&#39;,
 &#39;via&#39;,
 &#39;correct&#39;,
 &#39;eg&#39;,
 &#39;reading&#39;,
 &#39;course&#39;,
 &#39;enter&#39;,
 &#39;rock&#39;,
 &#39;rest&#39;,
 &#39;gone&#39;,
 &#39;offers&#39;,
 &#39;gettin&#39;,
 &#39;bank&#39;,
 &#39;pass&#39;,
 &#39;normal&#39;,
 &#39;press&#39;,
 &#39;wap&#39;,
 &#39;terms&#39;,
 &#39;nope&#39;,
 &#39;extra&#39;,
 &#39;move&#39;,
 &#39;woke&#39;,
 &#39;gave&#39;,
 &#39;within&#39;,
 &#39;charged&#39;,
 &#39;cut&#39;,
 &#39;through&#39;,
 &#39;wit&#39;,
 &#39;study&#39;,
 &#39;confirm&#39;,
 &#39;march&#39;,
 &#39;wkly&#39;,
 &#39;xy&#39;,
 &#39;ntt&#39;,
 &#39;reward&#39;,
 &#39;fact&#39;,
 &#39;understand&#39;,
 &#39;spend&#39;,
 &#39;mates&#39;,
 &#39;safe&#39;,
 &#39;ish&#39;,
 &#39;supposed&#39;,
 &#39;secret&#39;,
 &#39;meant&#39;,
 &#39;swing&#39;,
 &#39;comin&#39;,
 &#39;ass&#39;,
 &#39;invited&#39;,
 &#39;die&#39;,
 &#39;caller&#39;,
 &#39;askd&#39;,
 &#39;ans&#39;,
 &#39;hold&#39;,
 &#39;darren&#39;,
 &#39;cheap&#39;,
 &#39;mobiles&#39;,
 &#39;redeemed&#39;,
 &#39;sending&#39;,
 &#39;semester&#39;,
 &#39;mu&#39;,
 &#39;hungry&#39;,
 &#39;direct&#39;,
 &#39;checking&#39;,
 &#39;water&#39;,
 &#39;second&#39;,
 &#39;truth&#39;,
 &#39;wana&#39;,
 &#39;suite&#39;,
 &#39;wow&#39;,
 &#39;park&#39;,
 &#39;txts&#39;,
 &#39;ipod&#39;,
 &#39;slowly&#39;,
 &#39;heard&#39;,
 &#39;complimentary&#39;,
 &#39;moment&#39;,
 &#39;ugh&#39;,
 &#39;whole&#39;,
 &#39;warm&#39;,
 &#39;paper&#39;,
 &#39;hospital&#39;,
 &#39;own&#39;,
 &#39;brings&#39;,
 &#39;loved&#39;,
 &#39;ge&#39;,
 &#39;balance&#39;,
 &#39;savamob&#39;,
 &#39;merry&#39;,
 &#39;ringtones&#39;,
 &#39;stupid&#39;,
 &#39;gets&#39;,
 &#39;hee&#39;,
 &#39;children&#39;,
 &#39;charity&#39;,
 &#39;sound&#39;,
 &#39;comp&#39;,
 &#39;link&#39;,
 &#39;il&#39;,
 &#39;project&#39;,
 &#39;representative&#39;,
 &#39;cr&#39;,
 &#39;listen&#39;,
 &#39;laugh&#39;,
 &#39;mr&#39;,
 &#39;loads&#39;,
 &#39;frm&#39;,
 &#39;access&#39;,
 &#39;knows&#39;,
 &#39;immediately&#39;,
 &#39;ho&#39;,
 &#39;nyt&#39;,
 &#39;store&#39;,
 &#39;somebody&#39;,
 &#39;yep&#39;,
 &#39;kate&#39;,
 &#39;india&#39;,
 &#39;couple&#39;,
 &#39;valued&#39;,
 &#39;ends&#39;,
 &#39;bill&#39;,
 &#39;questions&#39;,
 &#39;worries&#39;,
 &#39;point&#39;,
 &#39;isn&#39;,
 &#39;leaves&#39;,
 &#39;dogging&#39;,
 &#39;fri&#39;,
 &#39;usual&#39;,
 &#39;pound&#39;,
 &#39;read&#39;,
 &#39;rental&#39;,
 &#39;ts&#39;,
 &#39;max&#39;,
 &#39;boss&#39;,
 &#39;ac&#39;,
 &#39;completely&#39;,
 &#39;em&#39;,
 &#39;buying&#39;,
 &#39;gotta&#39;,
 &#39;weed&#39;,
 &#39;film&#39;,
 &#39;crave&#39;,
 &#39;feels&#39;,
 &#39;kids&#39;,
 &#39;colleagues&#39;,
 &#39;thinks&#39;,
 &#39;mm&#39;,
 &#39;mrt&#39;,
 &#39;talking&#39;,
 &#39;entered&#39;,
 &#39;reveal&#39;,
 &#39;la&#39;,
 &#39;rakhesh&#39;,
 &#39;blood&#39;,
 &#39;rply&#39;,
 &#39;fantastic&#39;,
 &#39;asking&#39;,
 &#39;anymore&#39;,
 &#39;hw&#39;,
 &#39;difficult&#39;,
 &#39;user&#39;,
 &#39;asap&#39;,
 &#39;wasn&#39;,
 &#39;shower&#39;,
 &#39;whatever&#39;,
 &#39;train&#39;,
 &#39;sofa&#39;,
 &#39;catch&#39;,
 &#39;hiya&#39;,
 &#39;energy&#39;,
 &#39;motorola&#39;,
 &#39;none&#39;,
 &#39;slave&#39;,
 &#39;possible&#39;,
 &#39;thru&#39;,
 &#39;ex&#39;,
 &#39;moon&#39;,
 &#39;awaiting&#39;,
 &#39;muz&#39;,
 &#39;vl&#39;,
 &#39;luck&#39;,
 &#39;tickets&#39;,
 &#39;iam&#39;,
 &#39;bslvyl&#39;,
 &#39;less&#39;,
 &#39;different&#39;,
 &#39;bedroom&#39;,
 &#39;wc&#39;,
 &#39;workin&#39;,
 &#39;clean&#39;,
 &#39;dead&#39;,
 &#39;whenever&#39;,
 &#39;registered&#39;,
 &#39;wine&#39;,
 &#39;yar&#39;,
 &#39;abiola&#39;,
 &#39;kick&#39;,
 &#39;meh&#39;,
 &#39;txting&#39;,
 &#39;door&#39;,
 &#39;goodnight&#39;,
 &#39;deal&#39;,
 &#39;noon&#39;,
 &#39;sale&#39;,
 &#39;doesnt&#39;,
 &#39;lovable&#39;,
 &#39;dnt&#39;,
 &#39;jay&#39;,
 &#39;slept&#39;,
 &#39;yrs&#39;,
 &#39;convey&#39;,
 ...]
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate aggregated sentence vectors based on the word vectors for each word in the sentence</span>
<span class="n">w2v_vect</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ls</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">])</span>
                     <span class="k">for</span> <span class="n">ls</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Why is the length of the sentence different than the length of the sentence vector?</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">w2v_vect</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>14 14
7 7
6 6
13 13
8 7
16 15
6 4
17 15
7 6
6 6
6 6
26 23
4 4
25 25
11 11
22 22
19 14
27 27
10 9
29 28
2 2
7 7
17 17
8 8
4 4
9 9
6 5
11 11
5 4
9 7
21 16
10 10
9 9
27 27
8 8
6 3
7 6
2 2
24 19
29 26
9 8
21 19
6 5
6 6
22 18
24 21
11 10
7 6
33 31
7 7
41 36
20 19
22 18
23 23
17 17
9 8
6 6
16 16
12 12
11 10
41 34
5 5
3 3
24 23
18 16
9 9
8 7
30 30
23 23
8 8
29 27
33 32
10 10
12 12
29 29
5 5
13 13
4 3
6 6
24 24
16 15
3 1
9 9
15 11
15 13
12 11
7 7
11 11
6 6
1 1
19 18
13 12
6 6
14 14
5 4
23 23
8 7
25 24
17 17
4 2
17 17
10 9
5 5
10 9
13 11
6 6
9 7
17 16
14 14
13 10
32 31
23 22
22 21
23 22
9 9
18 16
21 18
7 7
16 12
22 22
16 16
8 7
8 8
24 14
30 25
8 8
25 19
6 6
5 5
5 5
2 2
7 7
29 28
5 5
8 8
6 6
25 25
26 25
23 23
8 8
13 13
22 22
13 12
8 7
5 5
15 15
14 14
13 12
11 9
31 31
23 21
36 35
7 5
30 30
4 4
9 6
15 14
7 6
16 16
1 1
10 8
19 18
25 20
7 7
16 12
3 2
44 41
8 5
17 16
81 79
11 11
47 40
6 5
73 69
7 7
7 7
14 13
20 19
16 12
11 10
9 8
18 17
11 11
17 17
7 4
7 6
22 21
17 17
7 7
23 21
4 4
5 5
3 3
5 5
9 7
3 3
12 11
9 9
26 25
29 26
8 5
7 7
16 15
25 24
3 3
10 10
16 14
17 16
27 26
3 3
19 16
9 9
4 4
10 9
11 10
19 19
20 19
32 31
2 2
22 20
27 25
5 5
9 7
4 4
23 22
5 4
29 28
11 9
5 5
6 6
31 31
9 9
7 6
19 18
3 2
11 11
21 18
6 5
3 2
5 5
21 21
8 7
21 19
16 15
15 13
8 8
6 6
22 20
21 18
5 4
6 6
14 14
27 26
17 16
9 9
10 10
18 18
6 6
20 18
27 22
26 23
24 22
24 24
1 1
19 12
9 9
12 10
14 13
7 7
8 8
25 25
5 5
9 7
29 26
3 3
7 6
18 16
19 19
22 22
30 29
8 6
5 5
8 8
15 12
6 5
14 14
5 5
7 6
14 11
6 6
42 35
11 10
8 8
26 24
8 8
10 10
18 15
7 7
11 11
7 7
6 6
26 24
28 23
8 7
28 26
17 16
19 14
24 20
9 7
5 5
5 5
4 4
20 20
4 4
14 13
5 5
18 13
14 14
7 7
14 12
6 2
8 8
29 29
15 15
14 13
8 6
12 12
5 4
14 14
10 10
2 2
8 8
12 11
21 21
33 29
27 26
33 33
23 22
5 4
9 8
3 2
24 24
8 8
26 21
5 5
16 10
3 2
5 5
25 25
4 4
17 15
8 7
20 19
9 9
23 19
20 20
31 29
17 16
7 7
6 6
7 7
16 16
12 12
7 7
19 15
5 5
29 28
29 29
21 20
18 17
22 20
8 8
8 8
23 18
13 13
8 5
23 23
5 5
24 22
33 33
6 5
23 22
3 3
7 7
7 6
19 17
3 3
8 8
10 10
7 6
7 6
10 10
26 25
79 75
7 6
11 11
10 9
10 9
22 22
15 14
23 23
9 8
8 3
3 3
13 11
16 15
7 7
7 5
22 21
7 6
15 15
27 22
8 6
15 14
1 1
26 23
19 15
9 9
12 11
23 19
8 8
7 7
4 3
13 13
21 19
16 15
18 16
11 9
5 5
9 7
9 9
5 4
11 11
12 11
24 21
6 5
15 14
4 4
25 25
7 5
6 5
10 9
8 8
19 19
23 19
7 5
45 44
29 28
8 8
16 16
12 12
2 2
20 20
7 6
31 29
18 18
18 18
31 25
22 21
11 10
17 12
8 8
6 6
23 19
10 9
19 19
3 3
6 6
10 9
20 17
14 12
18 14
12 12
19 19
16 14
23 23
9 8
6 6
20 17
10 6
29 29
28 28
15 15
5 5
4 4
7 6
5 5
12 11
6 5
11 10
7 6
6 4
10 10
1 1
5 5
16 16
8 8
12 11
5 5
21 21
4 4
38 35
6 5
13 12
24 24
18 17
33 29
10 8
21 21
4 4
13 12
5 5
5 4
22 21
4 4
7 7
16 14
27 27
25 23
14 14
11 11
6 6
11 11
4 4
25 21
21 18
19 19
5 5
14 11
14 14
5 5
31 30
24 22
20 17
24 20
6 6
13 11
5 4
15 14
30 29
18 17
9 5
20 20
9 9
3 3
19 17
7 7
5 5
29 27
9 9
23 13
26 26
23 22
8 6
6 5
26 25
3 3
6 6
17 16
8 8
12 12
6 6
7 6
8 2
5 5
8 7
14 14
15 15
26 24
4 4
12 11
31 29
34 33
6 6
13 13
6 6
1 1
12 9
5 5
20 17
3 3
8 6
18 14
24 23
5 4
15 14
24 24
13 12
50 45
6 4
27 27
8 8
23 19
9 9
17 13
7 7
23 20
9 7
13 11
5 4
24 24
8 7
9 8
6 6
5 5
6 5
21 9
5 3
15 15
11 9
8 8
10 10
2 2
5 2
0 0
10 9
9 9
7 5
26 25
9 9
20 18
4 2
6 5
25 20
27 26
12 12
35 34
9 5
15 15
30 30
26 24
5 5
9 9
13 12
4 3
12 7
13 12
16 14
8 8
5 5
30 22
21 20
23 21
24 24
12 11
5 5
20 19
21 18
10 9
10 9
14 13
11 10
21 18
5 5
9 7
5 4
24 21
46 40
7 7
4 4
10 8
7 5
7 7
16 16
28 24
7 7
15 13
9 9
4 4
8 8
7 6
8 8
16 16
24 22
27 27
27 27
8 7
24 20
5 4
10 10
13 13
6 6
26 26
5 5
11 11
3 3
6 4
9 9
25 22
7 6
50 44
8 8
44 42
5 3
6 6
27 22
8 7
15 14
21 21
9 9
15 14
9 9
9 8
6 6
22 19
20 20
1 1
30 30
23 22
10 10
110 82
6 5
7 7
22 22
15 14
22 22
4 2
11 8
18 17
1 1
4 4
46 40
11 11
6 6
10 8
9 9
10 10
29 27
4 4
25 25
13 13
6 6
26 22
26 24
15 11
23 23
68 63
31 29
17 16
20 18
4 4
7 6
26 23
4 4
5 4
11 11
13 12
22 19
9 7
10 8
7 7
8 7
13 10
9 8
5 5
9 9
5 5
5 5
6 6
6 4
9 9
20 14
6 6
13 12
24 21
7 7
26 19
29 28
12 11
6 6
15 12
16 14
5 5
8 8
17 16
23 21
6 5
7 6
6 6
14 14
1 1
25 23
30 28
11 10
30 30
13 13
17 17
9 9
20 19
18 17
20 18
6 6
22 22
1 1
20 19
10 9
8 7
5 5
31 26
5 5
18 17
9 8
75 66
4 4
6 6
6 6
6 6
28 28
11 11
12 12
21 21
25 24
19 14
29 28
5 4
12 11
18 17
5 5
14 14
20 17
8 8
11 11
23 23
10 10
21 19
12 11
21 21
5 3
28 27
7 5
7 5
8 8
4 4
9 8
8 7
14 14
8 8
21 20
17 16
29 29
13 9
13 12
5 5
21 21
22 22
17 16
5 5
3 3
9 8
26 24
7 7
8 7
17 17
10 9
17 14
16 15
5 5
3 2
17 15
21 20
14 12
22 17
23 20
16 15
6 6
17 17
21 21
8 6
15 10
9 9
4 4
7 7
25 19
14 14
30 29
10 9
28 27
8 5
6 6
4 4
4 3
15 10
1 1
4 4
19 18
4 4
25 23
10 10
12 12
20 19
19 17
6 5
13 13
5 5
6 5
42 35
14 11
12 9
10 8
14 14
6 6
15 14
17 17
14 14
14 13
12 11
22 20
18 17
9 7
17 17
22 21
28 24
24 24
18 17
8 6
5 5
6 4
4 4
21 21
7 6
22 21
5 4
18 14
6 6
7 7
20 19
29 26
7 5
7 7
26 21
17 17
6 4
27 26
16 14
17 14
22 20
3 1
5 5
10 10
17 13
27 27
8 7
6 5
28 23
20 18
15 15
22 15
17 16
12 12
10 10
10 8
5 4
24 22
23 17
5 5
24 24
4 4
27 24
5 4
25 22
26 24
9 9
13 13
4 3
19 19
27 26
14 11
8 7
10 10
10 10
13 13
10 7
5 5
3 2
6 6
12 11
23 22
9 9
16 16
29 27
17 17
8 4
13 9
38 31
26 26
4 4
5 5
12 12
18 16
17 17
20 17
6 6
15 14
15 15
27 6
12 11
8 8
31 29
6 5
4 4
22 11
42 39
10 8
47 32
4 4
21 19
5 5
22 21
30 30
4 4
7 6
11 10
16 16
8 8
6 5
9 9
16 16
9 9
10 9
7 7
24 23
5 5
13 12
4 4
7 6
7 6
20 19
10 10
65 59
4 1
37 29
16 13
25 25
5 3
9 8
13 11
6 5
23 20
15 15
23 19
16 16
19 17
24 22
9 9
5 4
9 9
27 25
4 1
5 5
28 27
7 6
8 8
3 3
6 6
19 18
6 4
15 14
23 23
1 1
9 9
6 5
13 13
5 5
16 11
9 8
8 8
18 16
4 4
7 6
14 14
16 14
7 7
8 8
12 12
18 18
6 6
22 17
14 13
5 3
5 4
21 21
8 8
5 5
25 23
20 16
8 6
14 11
8 7
6 5
16 16
16 16
21 18
13 11
10 8
5 4
8 7
26 25
20 20
13 13
6 6
61 56
11 10
4 4
8 7
7 7
21 21
5 5
22 20
10 9
33 31
9 9
47 46
23 18
16 16
25 21
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute sentence vectors by averaging the word vectors for the words contained in the sentence</span>
<span class="n">w2v_vect_avg</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">vect</span> <span class="ow">in</span> <span class="n">w2v_vect</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vect</span><span class="p">)</span><span class="o">!=</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">w2v_vect_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w2v_vect_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Are our sentence vector lengths consistent?</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">w2v_vect_avg</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>14 100
7 100
6 100
13 100
8 100
16 100
6 100
17 100
7 100
6 100
6 100
26 100
4 100
25 100
11 100
22 100
19 100
27 100
10 100
29 100
2 100
7 100
17 100
8 100
4 100
9 100
6 100
11 100
5 100
9 100
21 100
10 100
9 100
27 100
8 100
6 100
7 100
2 100
24 100
29 100
9 100
21 100
6 100
6 100
22 100
24 100
11 100
7 100
33 100
7 100
41 100
20 100
22 100
23 100
17 100
9 100
6 100
16 100
12 100
11 100
41 100
5 100
3 100
24 100
18 100
9 100
8 100
30 100
23 100
8 100
29 100
33 100
10 100
12 100
29 100
5 100
13 100
4 100
6 100
24 100
16 100
3 100
9 100
15 100
15 100
12 100
7 100
11 100
6 100
1 100
19 100
13 100
6 100
14 100
5 100
23 100
8 100
25 100
17 100
4 100
17 100
10 100
5 100
10 100
13 100
6 100
9 100
17 100
14 100
13 100
32 100
23 100
22 100
23 100
9 100
18 100
21 100
7 100
16 100
22 100
16 100
8 100
8 100
24 100
30 100
8 100
25 100
6 100
5 100
5 100
2 100
7 100
29 100
5 100
8 100
6 100
25 100
26 100
23 100
8 100
13 100
22 100
13 100
8 100
5 100
15 100
14 100
13 100
11 100
31 100
23 100
36 100
7 100
30 100
4 100
9 100
15 100
7 100
16 100
1 100
10 100
19 100
25 100
7 100
16 100
3 100
44 100
8 100
17 100
81 100
11 100
47 100
6 100
73 100
7 100
7 100
14 100
20 100
16 100
11 100
9 100
18 100
11 100
17 100
7 100
7 100
22 100
17 100
7 100
23 100
4 100
5 100
3 100
5 100
9 100
3 100
12 100
9 100
26 100
29 100
8 100
7 100
16 100
25 100
3 100
10 100
16 100
17 100
27 100
3 100
19 100
9 100
4 100
10 100
11 100
19 100
20 100
32 100
2 100
22 100
27 100
5 100
9 100
4 100
23 100
5 100
29 100
11 100
5 100
6 100
31 100
9 100
7 100
19 100
3 100
11 100
21 100
6 100
3 100
5 100
21 100
8 100
21 100
16 100
15 100
8 100
6 100
22 100
21 100
5 100
6 100
14 100
27 100
17 100
9 100
10 100
18 100
6 100
20 100
27 100
26 100
24 100
24 100
1 100
19 100
9 100
12 100
14 100
7 100
8 100
25 100
5 100
9 100
29 100
3 100
7 100
18 100
19 100
22 100
30 100
8 100
5 100
8 100
15 100
6 100
14 100
5 100
7 100
14 100
6 100
42 100
11 100
8 100
26 100
8 100
10 100
18 100
7 100
11 100
7 100
6 100
26 100
28 100
8 100
28 100
17 100
19 100
24 100
9 100
5 100
5 100
4 100
20 100
4 100
14 100
5 100
18 100
14 100
7 100
14 100
6 100
8 100
29 100
15 100
14 100
8 100
12 100
5 100
14 100
10 100
2 100
8 100
12 100
21 100
33 100
27 100
33 100
23 100
5 100
9 100
3 100
24 100
8 100
26 100
5 100
16 100
3 100
5 100
25 100
4 100
17 100
8 100
20 100
9 100
23 100
20 100
31 100
17 100
7 100
6 100
7 100
16 100
12 100
7 100
19 100
5 100
29 100
29 100
21 100
18 100
22 100
8 100
8 100
23 100
13 100
8 100
23 100
5 100
24 100
33 100
6 100
23 100
3 100
7 100
7 100
19 100
3 100
8 100
10 100
7 100
7 100
10 100
26 100
79 100
7 100
11 100
10 100
10 100
22 100
15 100
23 100
9 100
8 100
3 100
13 100
16 100
7 100
7 100
22 100
7 100
15 100
27 100
8 100
15 100
1 100
26 100
19 100
9 100
12 100
23 100
8 100
7 100
4 100
13 100
21 100
16 100
18 100
11 100
5 100
9 100
9 100
5 100
11 100
12 100
24 100
6 100
15 100
4 100
25 100
7 100
6 100
10 100
8 100
19 100
23 100
7 100
45 100
29 100
8 100
16 100
12 100
2 100
20 100
7 100
31 100
18 100
18 100
31 100
22 100
11 100
17 100
8 100
6 100
23 100
10 100
19 100
3 100
6 100
10 100
20 100
14 100
18 100
12 100
19 100
16 100
23 100
9 100
6 100
20 100
10 100
29 100
28 100
15 100
5 100
4 100
7 100
5 100
12 100
6 100
11 100
7 100
6 100
10 100
1 100
5 100
16 100
8 100
12 100
5 100
21 100
4 100
38 100
6 100
13 100
24 100
18 100
33 100
10 100
21 100
4 100
13 100
5 100
5 100
22 100
4 100
7 100
16 100
27 100
25 100
14 100
11 100
6 100
11 100
4 100
25 100
21 100
19 100
5 100
14 100
14 100
5 100
31 100
24 100
20 100
24 100
6 100
13 100
5 100
15 100
30 100
18 100
9 100
20 100
9 100
3 100
19 100
7 100
5 100
29 100
9 100
23 100
26 100
23 100
8 100
6 100
26 100
3 100
6 100
17 100
8 100
12 100
6 100
7 100
8 100
5 100
8 100
14 100
15 100
26 100
4 100
12 100
31 100
34 100
6 100
13 100
6 100
1 100
12 100
5 100
20 100
3 100
8 100
18 100
24 100
5 100
15 100
24 100
13 100
50 100
6 100
27 100
8 100
23 100
9 100
17 100
7 100
23 100
9 100
13 100
5 100
24 100
8 100
9 100
6 100
5 100
6 100
21 100
5 100
15 100
11 100
8 100
10 100
2 100
5 100
0 100
10 100
9 100
7 100
26 100
9 100
20 100
4 100
6 100
25 100
27 100
12 100
35 100
9 100
15 100
30 100
26 100
5 100
9 100
13 100
4 100
12 100
13 100
16 100
8 100
5 100
30 100
21 100
23 100
24 100
12 100
5 100
20 100
21 100
10 100
10 100
14 100
11 100
21 100
5 100
9 100
5 100
24 100
46 100
7 100
4 100
10 100
7 100
7 100
16 100
28 100
7 100
15 100
9 100
4 100
8 100
7 100
8 100
16 100
24 100
27 100
27 100
8 100
24 100
5 100
10 100
13 100
6 100
26 100
5 100
11 100
3 100
6 100
9 100
25 100
7 100
50 100
8 100
44 100
5 100
6 100
27 100
8 100
15 100
21 100
9 100
15 100
9 100
9 100
6 100
22 100
20 100
1 100
30 100
23 100
10 100
110 100
6 100
7 100
22 100
15 100
22 100
4 100
11 100
18 100
1 100
4 100
46 100
11 100
6 100
10 100
9 100
10 100
29 100
4 100
25 100
13 100
6 100
26 100
26 100
15 100
23 100
68 100
31 100
17 100
20 100
4 100
7 100
26 100
4 100
5 100
11 100
13 100
22 100
9 100
10 100
7 100
8 100
13 100
9 100
5 100
9 100
5 100
5 100
6 100
6 100
9 100
20 100
6 100
13 100
24 100
7 100
26 100
29 100
12 100
6 100
15 100
16 100
5 100
8 100
17 100
23 100
6 100
7 100
6 100
14 100
1 100
25 100
30 100
11 100
30 100
13 100
17 100
9 100
20 100
18 100
20 100
6 100
22 100
1 100
20 100
10 100
8 100
5 100
31 100
5 100
18 100
9 100
75 100
4 100
6 100
6 100
6 100
28 100
11 100
12 100
21 100
25 100
19 100
29 100
5 100
12 100
18 100
5 100
14 100
20 100
8 100
11 100
23 100
10 100
21 100
12 100
21 100
5 100
28 100
7 100
7 100
8 100
4 100
9 100
8 100
14 100
8 100
21 100
17 100
29 100
13 100
13 100
5 100
21 100
22 100
17 100
5 100
3 100
9 100
26 100
7 100
8 100
17 100
10 100
17 100
16 100
5 100
3 100
17 100
21 100
14 100
22 100
23 100
16 100
6 100
17 100
21 100
8 100
15 100
9 100
4 100
7 100
25 100
14 100
30 100
10 100
28 100
8 100
6 100
4 100
4 100
15 100
1 100
4 100
19 100
4 100
25 100
10 100
12 100
20 100
19 100
6 100
13 100
5 100
6 100
42 100
14 100
12 100
10 100
14 100
6 100
15 100
17 100
14 100
14 100
12 100
22 100
18 100
9 100
17 100
22 100
28 100
24 100
18 100
8 100
5 100
6 100
4 100
21 100
7 100
22 100
5 100
18 100
6 100
7 100
20 100
29 100
7 100
7 100
26 100
17 100
6 100
27 100
16 100
17 100
22 100
3 100
5 100
10 100
17 100
27 100
8 100
6 100
28 100
20 100
15 100
22 100
17 100
12 100
10 100
10 100
5 100
24 100
23 100
5 100
24 100
4 100
27 100
5 100
25 100
26 100
9 100
13 100
4 100
19 100
27 100
14 100
8 100
10 100
10 100
13 100
10 100
5 100
3 100
6 100
12 100
23 100
9 100
16 100
29 100
17 100
8 100
13 100
38 100
26 100
4 100
5 100
12 100
18 100
17 100
20 100
6 100
15 100
15 100
27 100
12 100
8 100
31 100
6 100
4 100
22 100
42 100
10 100
47 100
4 100
21 100
5 100
22 100
30 100
4 100
7 100
11 100
16 100
8 100
6 100
9 100
16 100
9 100
10 100
7 100
24 100
5 100
13 100
4 100
7 100
7 100
20 100
10 100
65 100
4 100
37 100
16 100
25 100
5 100
9 100
13 100
6 100
23 100
15 100
23 100
16 100
19 100
24 100
9 100
5 100
9 100
27 100
4 100
5 100
28 100
7 100
8 100
3 100
6 100
19 100
6 100
15 100
23 100
1 100
9 100
6 100
13 100
5 100
16 100
9 100
8 100
18 100
4 100
7 100
14 100
16 100
7 100
8 100
12 100
18 100
6 100
22 100
14 100
5 100
5 100
21 100
8 100
5 100
25 100
20 100
8 100
14 100
8 100
6 100
16 100
16 100
21 100
13 100
10 100
5 100
8 100
26 100
20 100
13 100
6 100
61 100
11 100
4 100
8 100
7 100
21 100
5 100
22 100
10 100
33 100
9 100
47 100
23 100
16 100
25 100
</pre></div>
</div>
</section>
</section>
<section id="doc2vec">
<h2>doc2vec<a class="headerlink" href="#doc2vec" title="Permalink to this heading">#</a></h2>
<p>doc2vec is a shallow, 2-layer neural network that accepts a text corpus as an input, and it returns a set of vectors (also known as embeddings); each vector is a numeric representation of a given sentence, paragraph, or document.</p>
<section id="train-our-own-model-for-doc2vec">
<h3>Train our own model (for doc2vec)<a class="headerlink" href="#train-our-own-model-for-doc2vec" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read in data, clean it, and then split into train and test sets</span>
<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_colwidth&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./spam.csv&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;latin-1&#39;</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="n">messages</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Unnamed: 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 4&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">messages</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">messages</span><span class="p">[</span><span class="s1">&#39;text_clean&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">gensim</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">simple_preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">messages</span><span class="p">[</span><span class="s1">&#39;text_clean&#39;</span><span class="p">],</span>
                                                    <span class="n">messages</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create tagged document objects to prepare to train the model</span>
<span class="n">tagged_docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">doc2vec</span><span class="o">.</span><span class="n">TaggedDocument</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_train</span><span class="p">)]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Look at what a tagged document looks like</span>
<span class="n">tagged_docs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>TaggedDocument(words=[&#39;customer&#39;, &#39;place&#39;, &#39;will&#39;, &#39;call&#39;, &#39;you&#39;], tags=[0])
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train a basic doc2vec model</span>
<span class="n">d2v_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Doc2Vec</span><span class="p">(</span><span class="n">tagged_docs</span><span class="p">,</span>
                                  <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                  <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                  <span class="n">min_count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What happens if we pass in a single word like we did for word2vec?</span>
<span class="n">d2v_model</span><span class="o">.</span><span class="n">infer_vector</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&lt;ipython-input-31-81bc935a6094&gt; in &lt;module&gt;
      1 # What happens if we pass in a single word like we did for word2vec?
----&gt; 2 d2v_model.infer_vector(&#39;text&#39;)


/opt/anaconda3/lib/python3.8/site-packages/gensim/models/doc2vec.py in infer_vector(self, doc_words, alpha, min_alpha, epochs, steps)
    660         &quot;&quot;&quot;
    661         if isinstance(doc_words, string_types):
--&gt; 662             raise TypeError(&quot;Parameter doc_words of infer_vector() must be a list of strings (not a single string).&quot;)
    663 
    664         alpha = alpha or self.alpha


TypeError: Parameter doc_words of infer_vector() must be a list of strings (not a single string).
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What happens if we pass in a list of words?</span>
<span class="n">d2v_model</span><span class="o">.</span><span class="n">infer_vector</span><span class="p">([</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s1">&#39;am&#39;</span><span class="p">,</span> <span class="s1">&#39;learning&#39;</span><span class="p">,</span> <span class="s1">&#39;nlp&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>array([ 8.7209214e-03,  6.8003386e-03,  5.5952729e-03, -2.5766229e-03,
        1.0613035e-03, -5.7215425e-03, -1.6412719e-03,  8.1747035e-03,
        2.0160328e-03,  5.2075079e-03,  1.0162117e-02,  6.0933977e-03,
       -4.4398084e-03, -4.7933441e-04,  9.7119426e-03, -3.4393093e-03,
        4.7703446e-03,  4.0532406e-03, -6.3246160e-05, -2.0210417e-03,
        1.7368731e-03,  3.4384518e-03,  4.7413744e-03, -2.3204719e-03,
        5.5889967e-03, -2.0065054e-03,  3.3309052e-03, -2.0756924e-03,
       -8.9801941e-03,  4.3926477e-03, -4.4796076e-03, -4.1491329e-03,
       -1.0473067e-02, -2.9401588e-03, -4.6068169e-03, -1.4974561e-03,
        5.9022271e-04,  6.7550489e-03, -4.1296746e-04, -7.9409238e-03,
       -4.4584284e-03, -6.1357087e-03,  5.5976193e-03,  6.2380014e-03,
        7.1805198e-04,  2.6298431e-03,  6.6803750e-03, -6.1393570e-05,
       -6.8914797e-03, -8.8394256e-03, -2.6981505e-03,  6.0479343e-03,
       -5.8431751e-03,  4.6421327e-03,  6.5076486e-03,  9.4151357e-03,
        5.8190562e-03, -9.8730810e-03,  5.6908056e-03, -2.4419534e-03,
        5.6113712e-03,  4.0624849e-03,  1.0102064e-03, -7.4610240e-03,
       -1.3350280e-02, -2.9424776e-03,  4.6019370e-04,  2.4571500e-03,
        1.1266882e-02,  4.7475193e-03,  2.7180463e-03,  3.4195082e-03,
       -1.8446230e-03, -1.0672212e-03, -2.6155384e-03, -3.9139716e-03,
       -6.1091157e-03, -5.5098375e-03, -6.4904196e-03,  3.5323465e-04,
        3.0873776e-03, -7.7407500e-03,  2.9637001e-03, -9.4409613e-03,
        4.0558148e-03, -3.6703530e-03, -3.4753678e-03,  1.4557107e-03,
        8.4304521e-03,  4.3981737e-03,  1.4647675e-03, -2.3701063e-03,
       -3.2173330e-04,  2.2746627e-03, -6.0358881e-03,  1.9078508e-03,
        1.7012114e-03, -3.3947765e-03, -3.1590206e-03,  2.1669692e-04],
      dtype=float32)
</pre></div>
</div>
</section>
<section id="what-about-pre-trained-document-vectors">
<h3>What About Pre-trained Document Vectors?<a class="headerlink" href="#what-about-pre-trained-document-vectors" title="Permalink to this heading">#</a></h3>
<p>There are not as many options as there are for word vectors. There also is not an easy API to read these in like there is for <code class="docutils literal notranslate"><span class="pre">word2vec</span></code> so it is more time consuming.</p>
<p>Pre-trained vectors from training on Wikipedia and Associated Press News can be found <a class="reference external" href="https://github.com/jhlau/doc2vec">here</a>. Feel free to explore on your own!</p>
</section>
</section>
<section id="deep-learning-based-vector-model">
<h2>Deep Learning - based Vector model<a class="headerlink" href="#deep-learning-based-vector-model" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>When you hear the term deep learning, just think of a large deep neural net. Deep referes to the number of layers typically as so this kind of the popular term that’s been adopted in the press. I think of them as deep neural networks generally.</p>
</div></blockquote>
<p>- <strong>Jeff Dean</strong>, Google Senior Fellow in Systems and Infra. Group</p>
</section>
<section id="recurrent-neural-network-rnn">
<h2>Recurrent Neural Network (RNN)<a class="headerlink" href="#recurrent-neural-network-rnn" title="Permalink to this heading">#</a></h2>
<p>Pattern matchin through the connection of many very simple functions to create one very powerful function; <strong>this function has an understanding of the data’s sequential nature (using feedback loops that form a sense of memory)</strong></p>
<section id="implement-basic-rnn">
<h3>Implement basic RNN<a class="headerlink" href="#implement-basic-rnn" title="Permalink to this heading">#</a></h3>
<section id="read-clean-and-split-the-data">
<h4>Read, clean and split the data:<a class="headerlink" href="#read-clean-and-split-the-data" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read in data and split into training and test set</span>
<span class="c1"># NOTE: we are NOT cleaning the data</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_colwidth&#39;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./spam.csv&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;latin-1&#39;</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="n">messages</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Unnamed: 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 4&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">messages</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">messages</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;spam&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">messages</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span>
                                                    <span class="n">labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="prep-data-for-modelling">
<h4>Prep data for modelling:<a class="headerlink" href="#prep-data-for-modelling" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the tools we will need from keras</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize and fit the tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use that tokenizer to transform the text messages in the training and test sets</span>
<span class="n">X_train_seq</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_seq</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What do these sequences look like?</span>
<span class="n">X_train_seq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[3,
 17,
 1277,
 2566,
 72,
 2567,
 716,
 8,
 28,
 2568,
 171,
 7,
 806,
 1662,
 45,
 44,
 2,
 871,
 2569,
 15,
 2570,
 27,
 23,
 1131,
 26,
 2571,
 1662,
 2,
 633]
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pad the sequences so each sequence is the same length</span>
<span class="n">X_train_seq_padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_train_seq</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">X_test_seq_padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_test_seq</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What do these padded sequences look like?</span>
<span class="n">X_train_seq_padded</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,
         17, 1277, 2566,   72, 2567,  716,    8,   28, 2568,  171,    7,
        806, 1662,   45,   44,    2,  871, 2569,   15, 2570,   27,   23,
       1131,   26, 2571, 1662,    2,  633], dtype=int32)
</pre></div>
</div>
</section>
<section id="build-model">
<h4>Build model:<a class="headerlink" href="#build-model" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the tools needed from keras and define functions to calculate recall and precision</span>
<span class="kn">import</span> <span class="nn">keras.backend</span> <span class="k">as</span> <span class="nn">K</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="k">def</span> <span class="nf">recall_m</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">true_positives</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">possible_positives</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">recall</span> <span class="o">=</span> <span class="n">true_positives</span> <span class="o">/</span> <span class="p">(</span><span class="n">possible_positives</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">epsilon</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">recall</span>

<span class="k">def</span> <span class="nf">precision_m</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">true_positives</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">predicted_positives</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="n">true_positives</span> <span class="o">/</span> <span class="p">(</span><span class="n">predicted_positives</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">epsilon</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">precision</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct a simple RNN model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">index_word</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 32)          254848    
_________________________________________________________________
lstm_1 (LSTM)                (None, 32)                8320      
_________________________________________________________________
dense_2 (Dense)              (None, 32)                1056      
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 33        
=================================================================
Total params: 264,257
Trainable params: 264,257
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compile the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">precision_m</span><span class="p">,</span> <span class="n">recall_m</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the RNN model</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_seq_padded</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> 
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test_seq_padded</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
140/140 [==============================] - 5s 20ms/step - loss: 0.4555 - accuracy: 0.8589 - precision_m: 0.1295 - recall_m: 0.0910 - val_loss: 0.0603 - val_accuracy: 0.9874 - val_precision_m: 0.9800 - val_recall_m: 0.9359
Epoch 2/10
140/140 [==============================] - 2s 15ms/step - loss: 0.0450 - accuracy: 0.9913 - precision_m: 0.9821 - recall_m: 0.9585 - val_loss: 0.0473 - val_accuracy: 0.9874 - val_precision_m: 1.0000 - val_recall_m: 0.9183
Epoch 3/10
140/140 [==============================] - 2s 14ms/step - loss: 0.0183 - accuracy: 0.9963 - precision_m: 0.9942 - recall_m: 0.9802 - val_loss: 0.0273 - val_accuracy: 0.9910 - val_precision_m: 0.9850 - val_recall_m: 0.9609
Epoch 4/10
140/140 [==============================] - 2s 14ms/step - loss: 0.0096 - accuracy: 0.9976 - precision_m: 0.9793 - recall_m: 0.9721 - val_loss: 0.0277 - val_accuracy: 0.9937 - val_precision_m: 0.9964 - val_recall_m: 0.9681
Epoch 5/10
140/140 [==============================] - 2s 16ms/step - loss: 0.0026 - accuracy: 0.9992 - precision_m: 0.9962 - recall_m: 0.9931 - val_loss: 0.0315 - val_accuracy: 0.9928 - val_precision_m: 0.9907 - val_recall_m: 0.9681
Epoch 6/10
140/140 [==============================] - 2s 15ms/step - loss: 5.7281e-04 - accuracy: 1.0000 - precision_m: 0.9856 - recall_m: 0.9855 - val_loss: 0.0339 - val_accuracy: 0.9919 - val_precision_m: 0.9850 - val_recall_m: 0.9681
Epoch 7/10
140/140 [==============================] - 2s 15ms/step - loss: 6.3776e-04 - accuracy: 1.0000 - precision_m: 0.9966 - recall_m: 0.9966 - val_loss: 0.0408 - val_accuracy: 0.9919 - val_precision_m: 0.9964 - val_recall_m: 0.9568
Epoch 8/10
140/140 [==============================] - 2s 15ms/step - loss: 2.1764e-04 - accuracy: 1.0000 - precision_m: 0.9999 - recall_m: 0.9999 - val_loss: 0.0444 - val_accuracy: 0.9910 - val_precision_m: 0.9964 - val_recall_m: 0.9511
Epoch 9/10
140/140 [==============================] - 2s 15ms/step - loss: 1.8205e-04 - accuracy: 1.0000 - precision_m: 0.9999 - recall_m: 0.9999 - val_loss: 0.0462 - val_accuracy: 0.9910 - val_precision_m: 0.9964 - val_recall_m: 0.9511
Epoch 10/10
140/140 [==============================] - 2s 14ms/step - loss: 1.0950e-04 - accuracy: 1.0000 - precision_m: 0.9970 - recall_m: 0.9970 - val_loss: 0.0462 - val_accuracy: 0.9910 - val_precision_m: 0.9964 - val_recall_m: 0.9511
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the evaluation metrics by each epoch for the model to see if we are over or underfitting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;precision_m&#39;</span><span class="p">,</span> <span class="s1">&#39;recall_m&#39;</span><span class="p">]:</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Results for </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="_images/advanced_nlp_55_0.png" /></p>
<p><img alt="png" src="_images/advanced_nlp_55_1.png" /></p>
<p><img alt="png" src="_images/advanced_nlp_55_2.png" /></p>
</section>
</section>
</section>
<section id="comparing-the-nlp-techniques">
<h2>Comparing the NLP Techniques<a class="headerlink" href="#comparing-the-nlp-techniques" title="Permalink to this heading">#</a></h2>
<section id="preparing-the-data-for-modelling">
<h3>Preparing the data for modelling:<a class="headerlink" href="#preparing-the-data-for-modelling" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read in and clean data</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./spam.csv&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;latin-1&#39;</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="n">messages</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Unnamed: 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 4&quot;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">messages</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">messages</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">messages</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;spam&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">])</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;\W+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">text</span>

<span class="n">messages</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">clean_text</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">messages</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>clean_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>
      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>Ok lar... Joking wif u oni...</td>
      <td>[ok, lar, joking, wif, u, oni]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>
      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>U dun say so early hor... U c already then say...</td>
      <td>[u, dun, say, early, hor, u, c, already, say]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>Nah I don't think he goes to usf, he lives around here though</td>
      <td>[nah, dont, think, goes, usf, lives, around, though]</td>
    </tr>
  </tbody>
</table>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split data into train and test set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">messages</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">],</span>
                                                    <span class="n">messages</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What do the first ten messages in the training set look like?</span>
<span class="n">X_train</span><span class="p">[:]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>4081                                                                                                                                                                                      [check, rooms, befor, activities]
4455                                                                                                                                                                 [want, mapquest, something, look, usf, dogwood, drive]
1442                                                                                                                                                                                                [ok, askd, u, knw, tht]
4317                                                                                                                                                                                             [si, take, mokka, players]
4915                                                                                                                                                                                               [dropped, em, omw, back]
                                                                                                               ...                                                                                                         
298                                                                                                                                                                          [cant, pick, phone, right, pls, send, message]
1347                                                                                                                                                                                                   [doesnt, need, test]
5292                                                       [urgent, please, call, 09061213237, landline, å, 5000, cash, luxury, 4, canary, islands, holiday, await, collection, tcs, sae, po, box, 177, m227xy, 150ppm, 16]
710     [height, confidence, aeronautics, professors, wer, calld, amp, wer, askd, 2, sit, aeroplane, aftr, sat, wer, told, dat, plane, ws, made, students, dey, hurried, plane, bt, 1, didnt, move, saidif, made, students]
460                                                                                                                                                                                        [u, go, phone, gonna, die, stay]
Name: clean_text, Length: 4457, dtype: object
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What do the labels look like?</span>
<span class="n">y_train</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>4081    0
4455    0
1442    0
4317    0
4915    0
4199    0
3733    0
3921    0
244     0
2099    1
Name: label, dtype: int64
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s save the training and test sets to ensure we are using the same data for each model</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;./spam_X_train.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_test</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;./spam_X_test.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_train</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;./spam_y_train.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_test</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;./spam_y_test.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the cleaned training and test sets</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./spam_X_train.csv&#39;</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./spam_X_test.csv&#39;</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./spam_y_train.csv&#39;</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./spam_y_test.csv&#39;</span><span class="p">)</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clean_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>['check', 'rooms', 'befor', 'activities']</td>
    </tr>
    <tr>
      <th>1</th>
      <td>['want', 'mapquest', 'something', 'look', 'usf', 'dogwood', 'drive']</td>
    </tr>
    <tr>
      <th>2</th>
      <td>['ok', 'askd', 'u', 'knw', 'tht']</td>
    </tr>
    <tr>
      <th>3</th>
      <td>['si', 'take', 'mokka', 'players']</td>
    </tr>
    <tr>
      <th>4</th>
      <td>['dropped', 'em', 'omw', 'back']</td>
    </tr>
  </tbody>
</table>
</div>
</section>
<section id="build-model-on-tf-idf-vectors">
<h3>Build Model on TF-IDF Vectors<a class="headerlink" href="#build-model-on-tf-idf-vectors" title="Permalink to this heading">#</a></h3>
<section id="create-tf-idf-vectors">
<h4>Create TF-IDF Vectors<a class="headerlink" href="#create-tf-idf-vectors" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate and fit a TFIDF vectorizer and then use that trained vectorizer</span>
<span class="c1"># to transform the messages in the training and test sets</span>
<span class="n">tfidf_vect</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="n">tfidf_vect</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span>
<span class="n">X_train_vect</span> <span class="o">=</span> <span class="n">tfidf_vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span>
<span class="n">X_test_vect</span> <span class="o">=</span> <span class="n">tfidf_vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What words did the vectorizer learn?</span>
<span class="n">tfidf_vect</span><span class="o">.</span><span class="n">vocabulary_</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;check&#39;: 1879,
 &#39;rooms&#39;: 6196,
 &#39;befor&#39;: 1389,
 &#39;activities&#39;: 878,
 &#39;want&#39;: 7806,
 &#39;mapquest&#39;: 4643,
 &#39;something&#39;: 6702,
 &#39;look&#39;: 4476,
 &#39;usf&#39;: 7649,
 &#39;dogwood&#39;: 2560,
 &#39;drive&#39;: 2623,
 &#39;ok&#39;: 5251,
 &#39;askd&#39;: 1185,
 &#39;knw&#39;: 4233,
 &#39;tht&#39;: 7302,
 &#39;si&#39;: 6527,
 &#39;take&#39;: 7105,
 &#39;mokka&#39;: 4864,
 &#39;players&#39;: 5584,
 &#39;dropped&#39;: 2630,
 &#39;em&#39;: 2741,
 &#39;omw&#39;: 5273,
 &#39;back&#39;: 1285,
 &#39;come&#39;: 2039,
 &#39;tomorrow&#39;: 7385,
 &#39;di&#39;: 2451,
 &#39;tacos&#39;: 7095,
 &#39;rajas&#39;: 5920,
 &#39;burrito&#39;: 1668,
 &#39;right&#39;: 6155,
 &#39;oh&#39;: 5244,
 &#39;really&#39;: 5975,
 &#39;perform&#39;: 5483,
 &#39;write&#39;: 8060,
 &#39;paper&#39;: 5406,
 &#39;go&#39;: 3346,
 &#39;movie&#39;: 4913,
 &#39;home&#39;: 3692,
 &#39;midnight&#39;: 4771,
 &#39;huh&#39;: 3779,
 &#39;dont&#39;: 2575,
 &#39;remember&#39;: 6053,
 &#39;old&#39;: 5267,
 &#39;commercial&#39;: 2051,
 &#39;sms&#39;: 6658,
 &#39;services&#39;: 6411,
 &#39;inclusive&#39;: 3879,
 &#39;text&#39;: 7209,
 &#39;credits&#39;: 2210,
 &#39;pls&#39;: 5600,
 &#39;goto&#39;: 3403,
 &#39;wwwcomuknet&#39;: 8090,
 &#39;login&#39;: 4459,
 &#39;unsubscribe&#39;: 7604,
 &#39;stop&#39;: 6902,
 &#39;extra&#39;: 2902,
 &#39;charge&#39;: 1857,
 &#39;help08700469649&#39;: 3605,
 &#39;po&#39;: 5610,
 &#39;box420&#39;: 1564,
 &#39;ip4&#39;: 3968,
 &#39;5we&#39;: 623,
 &#39;new&#39;: 5059,
 &#39;theory&#39;: 7248,
 &#39;argument&#39;: 1148,
 &#39;wins&#39;: 7963,
 &#39;situation&#39;: 6581,
 &#39;loses&#39;: 4493,
 &#39;person&#39;: 5497,
 &#39;argue&#39;: 1147,
 &#39;ur&#39;: 7626,
 &#39;friends&#39;: 3191,
 &#39;kick&#39;: 4178,
 &#39;amp&#39;: 1038,
 &#39;say&#39;: 6306,
 &#39;im&#39;: 3847,
 &#39;always&#39;: 1024,
 &#39;correct&#39;: 2149,
 &#39;urgent&#39;: 7628,
 &#39;please&#39;: 5592,
 &#39;call&#39;: 1710,
 &#39;09061743810&#39;: 195,
 &#39;landline&#39;: 4278,
 &#39;abta&#39;: 839,
 &#39;complimentary&#39;: 2072,
 &#39;tenerife&#39;: 7192,
 &#39;holiday&#39;: 3687,
 &#39;5000&#39;: 591,
 &#39;cash&#39;: 1794,
 &#39;await&#39;: 1256,
 &#39;collection&#39;: 2028,
 &#39;sae&#39;: 6251,
 &#39;tcs&#39;: 7150,
 &#39;box&#39;: 1554,
 &#39;326&#39;: 487,
 &#39;cw25wx&#39;: 2275,
 &#39;150&#39;: 304,
 &#39;ppm&#39;: 5702,
 &#39;honeybee&#39;: 3701,
 &#39;said&#39;: 6258,
 &#39;sweetest&#39;: 7066,
 &#39;world&#39;: 8032,
 &#39;god&#39;: 3353,
 &#39;laughed&#39;: 4307,
 &#39;waitu&#39;: 7786,
 &#39;havnt&#39;: 3564,
 &#39;met&#39;: 4761,
 &#39;reading&#39;: 5964,
 &#39;msg&#39;: 4923,
 &#39;moral&#39;: 4891,
 &#39;even&#39;: 2831,
 &#39;crack&#39;: 2193,
 &#39;jokes&#39;: 4093,
 &#39;gmgngegn&#39;: 3343,
 &#39;hospital&#39;: 3726,
 &#39;da&#39;: 2279,
 &#39;return&#39;: 6133,
 &#39;evening&#39;: 2832,
 &#39;many&#39;: 4641,
 &#39;buy&#39;: 1678,
 &#39;tone&#39;: 7389,
 &#39;club&#39;: 1993,
 &#39;subs&#39;: 6972,
 &#39;expired&#39;: 2890,
 &#39;resub&#39;: 6125,
 &#39;reply&#39;: 6083,
 &#39;monoc&#39;: 4880,
 &#39;monos&#39;: 4881,
 &#39;polyc&#39;: 5641,
 &#39;polys&#39;: 5645,
 &#39;weekly&#39;: 7866,
 &#39;150p&#39;: 310,
 &#39;per&#39;: 5478,
 &#39;week&#39;: 7863,
 &#39;txt&#39;: 7527,
 &#39;free&#39;: 3164,
 &#39;stream&#39;: 6925,
 &#39;0871212025016&#39;: 91,
 &#39;tried&#39;: 7473,
 &#39;contact&#39;: 2113,
 &#39;offer&#39;: 5228,
 &#39;video&#39;: 7706,
 &#39;phone&#39;: 5524,
 &#39;750&#39;: 680,
 &#39;anytime&#39;: 1095,
 &#39;network&#39;: 5052,
 &#39;mins&#39;: 4797,
 &#39;half&#39;: 3509,
 &#39;price&#39;: 5763,
 &#39;rental&#39;: 6069,
 &#39;camcorder&#39;: 1741,
 &#39;08000930705&#39;: 44,
 &#39;delivery&#39;: 2402,
 &#39;wed&#39;: 7855,
 &#39;nope&#39;: 5137,
 &#39;going&#39;: 3364,
 &#39;pump&#39;: 5854,
 &#39;petrol&#39;: 5511,
 &#39;lor&#39;: 4488,
 &#39;like&#39;: 4389,
 &#39;rain&#39;: 5915,
 &#39;soon&#39;: 6718,
 &#39;1010&#39;: 259,
 &#39;make&#39;: 4617,
 &#39;sure&#39;: 7039,
 &#39;dat&#39;: 2316,
 &#39;ive&#39;: 4019,
 &#39;woken&#39;: 8003,
 &#39;well&#39;: 7880,
 &#39;shes&#39;: 6463,
 &#39;big&#39;: 1438,
 &#39;surprise&#39;: 7044,
 &#39;dude&#39;: 2648,
 &#39;afraid&#39;: 926,
 &#39;neighbors&#39;: 5041,
 &#39;didnt&#39;: 2463,
 &#39;pick&#39;: 5537,
 &#39;hear&#39;: 3577,
 &#39;knew&#39;: 4224,
 &#39;slept&#39;: 6617,
 &#39;late&#39;: 4297,
 &#39;yest&#39;: 8187,
 &#39;wake&#39;: 7787,
 &#39;guys&#39;: 3485,
 &#39;planning&#39;: 5577,
 &#39;coming&#39;: 2047,
 &#39;er&#39;: 2798,
 &#39;yeah&#39;: 8164,
 &#39;1526&#39;: 336,
 &#39;sorry&#39;: 6727,
 &#39;tell&#39;: 7178,
 &#39;pubcafe&#39;: 5847,
 &#39;sit&#39;: 6575,
 &#39;wen&#39;: 7885,
 &#39;sweet&#39;: 7065,
 &#39;heart&#39;: 3581,
 &#39;done&#39;: 2572,
 &#39;yo&#39;: 8197,
 &#39;byatch&#39;: 1690,
 &#39;whassup&#39;: 7903,
 &#39;charges&#39;: 1859,
 &#39;transfer&#39;: 7453,
 &#39;withdraw&#39;: 7977,
 &#39;anyhow&#39;: 1085,
 &#39;hai&#39;: 3503,
 &#39;ana&#39;: 1047,
 &#39;tomarrow&#39;: 7380,
 &#39;morning&#39;: 4895,
 &#39;ltdecimalgt&#39;: 4536,
 &#39;ill&#39;: 3844,
 &#39;sathy&#39;: 6291,
 &#39;rto&#39;: 6220,
 &#39;office&#39;: 5232,
 &#39;came&#39;: 1742,
 &#39;registered&#39;: 6031,
 &#39;subscriber&#39;: 6976,
 &#39;yr&#39;: 8227,
 &#39;draw&#39;: 2606,
 &#39;100&#39;: 251,
 &#39;gift&#39;: 3317,
 &#39;voucher&#39;: 7753,
 &#39;entered&#39;: 2781,
 &#39;receipt&#39;: 5990,
 &#39;ans&#39;: 1072,
 &#39;next&#39;: 5068,
 &#39;olympics&#39;: 5271,
 &#39;80062&#39;: 704,
 &#39;22&#39;: 400,
 &#39;days&#39;: 2334,
 &#39;euro2004&#39;: 2824,
 &#39;kept&#39;: 4167,
 &#39;date&#39;: 2318,
 &#39;latest&#39;: 4303,
 &#39;news&#39;: 5064,
 &#39;results&#39;: 6127,
 &#39;daily&#39;: 2290,
 &#39;removed&#39;: 6065,
 &#39;send&#39;: 6388,
 &#39;get&#39;: 3303,
 &#39;83222&#39;: 733,
 &#39;hex&#39;: 3630,
 &#39;place&#39;: 5566,
 &#39;talk&#39;: 7112,
 &#39;explain&#39;: 2893,
 &#39;haha&#39;: 3500,
 &#39;angry&#39;: 1056,
 &#39;practice&#39;: 5709,
 &#39;real&#39;: 5967,
 &#39;thing&#39;: 7265,
 &#39;lit&#39;: 4426,
 &#39;hair&#39;: 3504,
 &#39;fire&#39;: 3044,
 &#39;one&#39;: 5277,
 &#39;small&#39;: 6634,
 &#39;prestige&#39;: 5754,
 &#39;problem&#39;: 5788,
 &#39;doubt&#39;: 2593,
 &#39;could&#39;: 2168,
 &#39;handle&#39;: 3520,
 &#39;times&#39;: 7324,
 &#39;night&#39;: 5083,
 &#39;case&#39;: 1793,
 &#39;reach&#39;: 5957,
 &#39;orchard&#39;: 5321,
 &#39;already&#39;: 1013,
 &#39;wan&#39;: 7801,
 &#39;tickets&#39;: 7311,
 &#39;first&#39;: 3050,
 &#39;thinking&#39;: 7271,
 &#39;chennai&#39;: 1893,
 &#39;forgot&#39;: 3128,
 &#39;auction&#39;: 1223,
 &#39;callon&#39;: 1733,
 &#39;friday&#39;: 3187,
 &#39;assume&#39;: 1197,
 &#39;wont&#39;: 8012,
 &#39;year&#39;: 8166,
 &#39;squatting&#39;: 6819,
 &#39;way&#39;: 7837,
 &#39;walking&#39;: 7792,
 &#39;lovely&#39;: 4514,
 &#39;smell&#39;: 6642,
 &#39;bus&#39;: 1669,
 &#39;aint&#39;: 968,
 &#39;tobacco&#39;: 7356,
 &#39;great&#39;: 3433,
 &#39;hope&#39;: 3711,
 &#39;man&#39;: 4627,
 &#39;endowed&#39;: 2760,
 &#39;ltgt&#39;: 4540,
 &#39;inches&#39;: 3873,
 &#39;hey&#39;: 3631,
 &#39;dr&#39;: 2602,
 &#39;guess&#39;: 3473,
 &#39;wants&#39;: 7810,
 &#39;alone&#39;: 1011,
 &#39;time&#39;: 7321,
 &#39;show&#39;: 6511,
 &#39;watch&#39;: 7825,
 &#39;freemsg&#39;: 3172,
 &#39;hi&#39;: 3635,
 &#39;baby&#39;: 1279,
 &#39;wow&#39;: 8055,
 &#39;got&#39;: 3400,
 &#39;cam&#39;: 1740,
 &#39;moby&#39;: 4857,
 &#39;wanna&#39;: 7804,
 &#39;hot&#39;: 3731,
 &#39;pic&#39;: 5536,
 &#39;fancy&#39;: 2946,
 &#39;chatim&#39;: 1870,
 &#39;w8in&#39;: 7768,
 &#39;4utxt&#39;: 586,
 &#39;rply&#39;: 6212,
 &#39;chat&#39;: 1868,
 &#39;82242&#39;: 722,
 &#39;hlp&#39;: 3663,
 &#39;08712317606&#39;: 93,
 &#39;msg150p&#39;: 4924,
 &#39;2rcv&#39;: 457,
 &#39;whats&#39;: 7905,
 &#39;sleeping&#39;: 6610,
 &#39;aight&#39;: 967,
 &#39;youre&#39;: 8216,
 &#39;close&#39;: 1983,
 &#39;luck&#39;: 4544,
 &#39;love&#39;: 4512,
 &#39;someone&#39;: 6695,
 &#39;fortune&#39;: 3137,
 &#39;loves&#39;: 4520,
 &#39;good&#39;: 3378,
 &#39;dear&#39;: 2351,
 &#39;successful&#39;: 6984,
 &#39;day&#39;: 2329,
 &#39;thru&#39;: 7299,
 &#39;different&#39;: 2477,
 &#39;feelingwavering&#39;: 2983,
 &#39;decisions&#39;: 2371,
 &#39;coping&#39;: 2142,
 &#39;individualtime&#39;: 3897,
 &#39;heal&#39;: 3574,
 &#39;everything&#39;: 2844,
 &#39;believe&#39;: 1400,
 &#39;anyone&#39;: 1087,
 &#39;calls&#39;: 1734,
 &#39;treadmill&#39;: 7464,
 &#39;youll&#39;: 8212,
 &#39;working&#39;: 8029,
 &#39;found&#39;: 3144,
 &#39;ad&#39;: 883,
 &#39;craigslist&#39;: 2194,
 &#39;selling&#39;: 6382,
 &#39;trying&#39;: 7492,
 &#39;without&#39;: 7980,
 &#39;success&#39;: 6983,
 &#39;449071512431&#39;: 551,
 &#39;2nd&#39;: 449,
 &#39;attempt&#39;: 1214,
 &#39;uu&#39;: 7661,
 &#39;1250&#39;: 286,
 &#39;09071512433&#39;: 236,
 &#39;b4&#39;: 1269,
 &#39;050703&#39;: 12,
 &#39;tcsbcm4235wc1n3xx&#39;: 7151,
 &#39;callcost&#39;: 1721,
 &#39;150ppm&#39;: 324,
 &#39;mobilesvary&#39;: 4851,
 &#39;maxå&#39;: 4688,
 &#39;50&#39;: 589,
 &#39;probably&#39;: 5787,
 &#39;closer&#39;: 1986,
 &#39;jay&#39;: 4049,
 &#39;tyler&#39;: 7536,
 &#39;two&#39;: 7526,
 &#39;trips&#39;: 7476,
 &#39;erm&#39;: 2800,
 &#39;woodland&#39;: 8014,
 &#39;avenue&#39;: 1250,
 &#39;somewhere&#39;: 6707,
 &#39;parish&#39;: 5421,
 &#39;magazine&#39;: 4602,
 &#39;telephone&#39;: 7176,
 &#39;number&#39;: 5186,
 &#39;think&#39;: 7268,
 &#39;sent&#39;: 6398,
 &#39;cant&#39;: 1759,
 &#39;display&#39;: 2523,
 &#39;texts&#39;: 7217,
 &#39;still&#39;: 6891,
 &#39;leave&#39;: 4334,
 &#39;de&#39;: 2343,
 &#39;start&#39;: 6852,
 &#39;prepare&#39;: 5739,
 &#39;dnt&#39;: 2539,
 &#39;wnt&#39;: 7998,
 &#39;tlk&#39;: 7344,
 &#39;wid&#39;: 7935,
 &#39;hows&#39;: 3748,
 &#39;pain&#39;: 5387,
 &#39;deary&#39;: 2361,
 &#39;smiling&#39;: 6650,
 &#39;eastenders&#39;: 2683,
 &#39;tv&#39;: 7517,
 &#39;quiz&#39;: 5901,
 &#39;flower&#39;: 3084,
 &#39;dot&#39;: 2587,
 &#39;compare&#39;: 2058,
 &#39;violet&#39;: 7725,
 &#39;tulip&#39;: 7507,
 &#39;lily&#39;: 4395,
 &#39;84025&#39;: 743,
 &#39;chance&#39;: 1845,
 &#39;win&#39;: 7950,
 &#39;wkent150p16&#39;: 7987,
 &#39;cruisin&#39;: 2227,
 &#39;girl&#39;: 3325,
 &#39;friend&#39;: 3190,
 &#39;give&#39;: 3332,
 &#39;hour&#39;: 3736,
 &#39;thats&#39;: 7232,
 &#39;alright&#39;: 1015,
 &#39;fone&#39;: 3106,
 &#39;jenny&#39;: 4063,
 &#39;xxx&#39;: 8139,
 &#39;purity&#39;: 5864,
 &#39;friendship&#39;: 3193,
 &#39;forwarded&#39;: 3141,
 &#39;messageits&#39;: 4751,
 &#39;seeing&#39;: 6362,
 &#39;name&#39;: 4990,
 &#39;gud&#39;: 3468,
 &#39;evng&#39;: 2850,
 &#39;musthu&#39;: 4968,
 &#39;ac&#39;: 842,
 &#39;sun0819&#39;: 7012,
 &#39;posts&#39;: 5685,
 &#39;helloyou&#39;: 3603,
 &#39;seem&#39;: 6365,
 &#39;cool&#39;: 2136,
 &#39;fast&#39;: 2956,
 &#39;lose&#39;: 4491,
 &#39;weight&#39;: 7871,
 &#39;thk&#39;: 7275,
 &#39;muz&#39;: 4972,
 &#39;month&#39;: 4883,
 &#39;den&#39;: 2406,
 &#39;effect&#39;: 2708,
 &#39;geelater&#39;: 3287,
 &#39;aust&#39;: 1235,
 &#39;put&#39;: 5871,
 &#39;bk&#39;: 1463,
 &#39;intrepid&#39;: 3953,
 &#39;duo&#39;: 2658,
 &#39;see&#39;: 6360,
 &#39;smile&#39;: 6646,
 &#39;pleasure&#39;: 5595,
 &#39;trouble&#39;: 7479,
 &#39;pours&#39;: 5694,
 &#39;sum1&#39;: 7005,
 &#39;hurts&#39;: 3800,
 &#39;becoz&#39;: 1378,
 &#39;splashmobile&#39;: 6791,
 &#39;choose&#39;: 1937,
 &#39;1000s&#39;: 256,
 &#39;gr8&#39;: 3413,
 &#39;tones&#39;: 7390,
 &#39;wk&#39;: 7985,
 &#39;subscrition&#39;: 6981,
 &#39;service&#39;: 6410,
 &#39;costing&#39;: 2159,
 &#39;300p&#39;: 475,
 &#39;credit&#39;: 2208,
 &#39;enjoy&#39;: 2772,
 &#39;today&#39;: 7359,
 &#39;accept&#39;: 847,
 &#39;dayu&#39;: 2340,
 &#39;brother&#39;: 1623,
 &#39;sister&#39;: 6573,
 &#39;lover&#39;: 4516,
 &#39;dear1&#39;: 2352,
 &#39;best1&#39;: 1419,
 &#39;clos1&#39;: 1982,
 &#39;lvblefrnd&#39;: 4564,
 &#39;jstfrnd&#39;: 4111,
 &#39;cutefrnd&#39;: 2270,
 &#39;lifpartnr&#39;: 4383,
 &#39;belovd&#39;: 1407,
 &#39;swtheart&#39;: 7079,
 &#39;bstfrnd&#39;: 1635,
 &#39;means&#39;: 4702,
 &#39;enemy&#39;: 2763,
 &#39;ya&#39;: 8150,
 &#39;nice&#39;: 5074,
 &#39;ready&#39;: 5965,
 &#39;thursday&#39;: 7305,
 &#39;oredi&#39;: 5325,
 &#39;todays&#39;: 7365,
 &#39;shows&#39;: 6517,
 &#39;800&#39;: 702,
 &#39;prize&#39;: 5780,
 &#39;guaranteed&#39;: 3466,
 &#39;09050003091&#39;: 160,
 &#39;land&#39;: 4276,
 &#39;line&#39;: 4402,
 &#39;claim&#39;: 1956,
 &#39;c52&#39;: 1694,
 &#39;valid12hrs&#39;: 7673,
 &#39;dang&#39;: 2300,
 &#39;mean&#39;: 4699,
 &#39;lol&#39;: 4466,
 &#39;laptop&#39;: 4289,
 &#39;noe&#39;: 5113,
 &#39;infra&#39;: 3911,
 &#39;slow&#39;: 6630,
 &#39;lar&#39;: 4290,
 &#39;missed&#39;: 4814,
 &#39;havent&#39;: 3560,
 &#39;much&#39;: 4941,
 &#39;bit&#39;: 1459,
 &#39;bored&#39;: 1534,
 &#39;bak&#39;: 1298,
 &#39;college&#39;: 2029,
 &#39;sad&#39;: 6250,
 &#39;isnt&#39;: 3989,
 &#39;itxx&#39;: 4015,
 &#39;anyway&#39;: 1096,
 &#39;shopping&#39;: 6493,
 &#39;cos&#39;: 2155,
 &#39;sis&#39;: 6572,
 &#39;yet&#39;: 8189,
 &#39;dun&#39;: 2655,
 &#39;disturb&#39;: 2526,
 &#39;liao&#39;: 4367,
 &#39;spirit&#39;: 6786,
 &#39;bb&#39;: 1345,
 &#39;need&#39;: 5029,
 &#39;mobile&#39;: 4848,
 &#39;077xxx&#39;: 27,
 &#39;2000&#39;: 384,
 &#39;bonus&#39;: 1521,
 &#39;caller&#39;: 1726,
 &#39;020603&#39;: 5,
 &#39;09066362206&#39;: 223,
 &#39;asap&#39;: 1177,
 &#39;box97n7qp&#39;: 1568,
 &#39;england&#39;: 2770,
 &#39;official&#39;: 5237,
 &#39;poly&#39;: 5638,
 &#39;ringtone&#39;: 6162,
 &#39;colour&#39;: 2032,
 &#39;flag&#39;: 3062,
 &#39;yer&#39;: 8175,
 &#39;84199&#39;: 746,
 &#39;optout&#39;: 5314,
 &#39;eng&#39;: 2765,
 &#39;box39822&#39;: 1562,
 &#39;w111wx&#39;: 7761,
 &#39;hui&#39;: 3780,
 &#39;xin&#39;: 8129,
 &#39;lib&#39;: 4369,
 &#39;nokia&#39;: 5123,
 &#39;7250i&#39;: 674,
 &#39;part&#39;: 5426,
 &#39;86021&#39;: 755,
 &#39;hgsuite3422lands&#39;: 3632,
 &#39;roww1jhl&#39;: 6208,
 &#39;16&#39;: 340,
 &#39;okmail&#39;: 5257,
 &#39;dave&#39;: 2326,
 &#39;final&#39;: 3024,
 &#39;notice&#39;: 5155,
 &#39;collect&#39;: 2025,
 &#39;award&#39;: 1259,
 &#39;09061743806&#39;: 194,
 &#39;box326&#39;: 1559,
 &#39;aiyar&#39;: 975,
 &#39;meet&#39;: 4715,
 &#39;lunch&#39;: 4553,
 &#39;la&#39;: 4257,
 &#39;message&#39;: 4748,
 &#39;food&#39;: 3108,
 &#39;fridge&#39;: 3188,
 &#39;meal&#39;: 4697,
 &#39;tonight&#39;: 7396,
 &#39;sitting&#39;: 6580,
 &#39;mu&#39;: 4940,
 &#39;waiting&#39;: 7785,
 &#39;everyone&#39;: 2842,
 &#39;suite&#39;: 7000,
 &#39;shower&#39;: 6512,
 &#39;1st&#39;: 371,
 &#39;no1&#39;: 5103,
 &#39;every&#39;: 2837,
 &#39;8077&#39;: 715,
 &#39;txting&#39;: 7532,
 &#39;mates&#39;: 4671,
 &#39;wwwgetzedcouk&#39;: 8095,
 &#39;pobox&#39;: 5611,
 &#39;36504&#39;: 495,
 &#39;w45wq&#39;: 7767,
 &#39;norm150ptone&#39;: 5142,
 &#39;house&#39;: 3739,
 &#39;beer&#39;: 1385,
 &#39;height&#39;: 3593,
 &#39;shit&#39;: 6477,
 &#39;guy&#39;: 3484,
 &#39;throws&#39;: 7298,
 &#39;luv&#39;: 4558,
 &#39;letter&#39;: 4363,
 &#39;gal&#39;: 3244,
 &#39;falls&#39;: 2937,
 &#39;brothers&#39;: 1624,
 &#39;head&#39;: 3567,
 &#39;whos&#39;: 7928,
 &#39;gay&#39;: 3270,
 &#39;amk&#39;: 1031,
 &#39;drink&#39;: 2618,
 &#39;tea&#39;: 7155,
 &#39;coffee&#39;: 2014,
 &#39;happens&#39;: 3537,
 &#39;2waxsto&#39;: 464,
 &#39;wat&#39;: 7824,
 &#39;medical&#39;: 4710,
 &#39;insurance&#39;: 3936,
 &#39;shell&#39;: 6461,
 &#39;able&#39;: 832,
 &#39;deliver&#39;: 2399,
 &#39;basic&#39;: 1330,
 &#39;care&#39;: 1772,
 &#39;currently&#39;: 2257,
 &#39;til&#39;: 7318,
 &#39;major&#39;: 4616,
 &#39;guide&#39;: 3477,
 &#39;buff&#39;: 1646,
 &#39;wind&#39;: 7952,
 &#39;brainless&#39;: 1578,
 &#39;dolld&#39;: 2569,
 &#39;vehicle&#39;: 7689,
 &#39;sariyag&#39;: 6286,
 &#39;madoke&#39;: 4597,
 &#39;barolla&#39;: 1323,
 &#39;spatula&#39;: 6758,
 &#39;hands&#39;: 3521,
 &#39;told&#39;: 7375,
 &#39;accenture&#39;: 846,
 &#39;confirm&#39;: 2092,
 &#39;true&#39;: 7484,
 &#39;kkare&#39;: 4208,
 &#39;topicsorry&#39;: 7413,
 &#39;telling&#39;: 7179,
 &#39;drunk&#39;: 2637,
 &#39;ki&#39;: 4177,
 &#39;deleted&#39;: 2396,
 &#39;cut&#39;: 2268,
 &#39;short&#39;: 6498,
 &#39;leh&#39;: 4345,
 &#39;ah&#39;: 952,
 &#39;failed&#39;: 2923,
 &#39;quite&#39;: 5898,
 &#39;ranjith&#39;: 5936,
 &#39;cal&#39;: 1703,
 &#39;drpd&#39;: 2633,
 &#39;deeraj&#39;: 2381,
 &#39;deepak&#39;: 2379,
 &#39;5min&#39;: 614,
 &#39;hold&#39;: 3684,
 &#39;though&#39;: 7284,
 &#39;hm&#39;: 3664,
 &#39;wait&#39;: 7781,
 &#39;dunno&#39;: 2657,
 &#39;wot&#39;: 8048,
 &#39;hell&#39;: 3597,
 &#39;gonna&#39;: 3376,
 &#39;another&#39;: 1071,
 &#39;weeks&#39;: 7867,
 &#39;become&#39;: 1376,
 &#39;slob&#39;: 6627,
 &#39;sam&#39;: 6270,
 &#39;eachother&#39;: 2669,
 &#39;wah&#39;: 7774,
 &#39;okie&#39;: 5255,
 &#39;use&#39;: 7643,
 &#39;unlimited&#39;: 7596,
 &#39;voda&#39;: 7742,
 &#39;numbers&#39;: 5189,
 &#39;ending&#39;: 2758,
 &#39;7634&#39;: 683,
 &#39;selected&#39;: 6375,
 &#39;receive&#39;: 5992,
 &#39;350&#39;: 491,
 &#39;reward&#39;: 6143,
 &#39;match&#39;: 4668,
 &#39;08712300220&#39;: 92,
 &#39;quoting&#39;: 5906,
 &#39;code&#39;: 2012,
 &#39;7684&#39;: 684,
 &#39;standard&#39;: 6842,
 &#39;rates&#39;: 5940,
 &#39;apply&#39;: 1115,
 &#39;av&#39;: 1240,
 &#39;wil&#39;: 7945,
 &#39;oneta&#39;: 5280,
 &#39;ugh&#39;: 7546,
 &#39;apologize&#39;: 1107,
 &#39;admit&#39;: 901,
 &#39;wrong&#39;: 8067,
 &#39;ask&#39;: 1184,
 &#39;chuckin&#39;: 1947,
 &#39;red&#39;: 6011,
 &#39;green&#39;: 3439,
 &#39;black&#39;: 1464,
 &#39;trainners&#39;: 7448,
 &#39;save&#39;: 6301,
 &#39;carryin&#39;: 1789,
 &#39;bac&#39;: 1284,
 &#39;train&#39;: 7445,
 &#39;castor&#39;: 1801,
 &#39;poking&#39;: 5632,
 &#39;everyday&#39;: 2841,
 &#39;teach&#39;: 7156,
 &#39;canada&#39;: 1748,
 &#39;abi&#39;: 828,
 &#39;saying&#39;: 6307,
 &#39;account&#39;: 858,
 &#39;happy&#39;: 3542,
 &#39;babe&#39;: 1276,
 &#39;woo&#39;: 8013,
 &#39;hoo&#39;: 3705,
 &#39;party&#39;: 5431,
 &#39;thanks&#39;: 7225,
 &#39;continued&#39;: 2122,
 &#39;support&#39;: 7030,
 &#39;question&#39;: 5890,
 &#39;enter&#39;: 2780,
 &#39;in2&#39;: 3871,
 &#39;us&#39;: 7639,
 &#39;president&#39;: 5748,
 &#39;80082&#39;: 706,
 &#39;ultimately&#39;: 7558,
 &#39;tor&#39;: 7418,
 &#39;motive&#39;: 4905,
 &#39;tui&#39;: 7505,
 &#39;achieve&#39;: 863,
 &#39;korli&#39;: 4239,
 &#39;saw&#39;: 6305,
 &#39;ago&#39;: 948,
 &#39;sell&#39;: 6381,
 &#39;wifi&#39;: 7942,
 &#39;3g&#39;: 508,
 &#39;blanked&#39;: 1473,
 &#39;join&#39;: 4087,
 &#39;denis&#39;: 2408,
 &#39;mina&#39;: 4784,
 &#39;08715205273&#39;: 119,
 &#39;felt&#39;: 2989,
 &#39;sonot&#39;: 6714,
 &#39;conveying&#39;: 2129,
 &#39;reason&#39;: 5979,
 &#39;ese&#39;: 2812,
 &#39;took&#39;: 7402,
 &#39;long&#39;: 4472,
 &#39;mine&#39;: 4789,
 &#39;busy&#39;: 1674,
 &#39;game&#39;: 3251,
 &#39;work&#39;: 8025,
 &#39;called&#39;: 1725,
 &#39;mom&#39;: 4866,
 &#39;instead&#39;: 3934,
 &#39;fun&#39;: 3223,
 &#39;scared&#39;: 6315,
 &#39;finish&#39;: 3037,
 &#39;engin&#39;: 2769,
 &#39;arts&#39;: 1173,
 &#39;pretty&#39;: 5757,
 &#39;wif&#39;: 7937,
 &#39;hes&#39;: 3626,
 &#39;cutting&#39;: 2273,
 &#39;stupid&#39;: 6963,
 &#39;anything&#39;: 1092,
 &#39;dad&#39;: 2283,
 &#39;spoken&#39;: 6798,
 &#39;boy&#39;: 1569,
 &#39;words&#39;: 8022,
 &#39;left&#39;: 4340,
 &#39;sighs&#39;: 6537,
 &#39;goes&#39;: 3359,
 &#39;studying&#39;: 6956,
 &#39;tape&#39;: 7126,
 &#39;pple&#39;: 5700,
 &#39;type&#39;: 7537,
 &#39;help&#39;: 3604,
 &#39;hee&#39;: 3591,
 &#39;infernal&#39;: 3902,
 &#39;affairs&#39;: 920,
 &#39;along&#39;: 1012,
 &#39;asking&#39;: 1188,
 &#39;shuhui&#39;: 6523,
 &#39;oso&#39;: 5336,
 &#39;saturday&#39;: 6298,
 &#39;cherish&#39;: 1897,
 &#39;role&#39;: 6184,
 &#39;model&#39;: 4859,
 &#39;okay&#39;: 5252,
 &#39;weekend&#39;: 7864,
 &#39;getting&#39;: 3310,
 &#39;jacket&#39;: 4029,
 &#39;used&#39;: 7644,
 &#39;multis&#39;: 4950,
 &#39;class&#39;: 1964,
 &#39;ìï&#39;: 8255,
 &#39;solve&#39;: 6690,
 &#39;murdered&#39;: 4960,
 &#39;afternoon&#39;: 931,
 &#39;1his&#39;: 359,
 &#39;wife&#39;: 7938,
 &#39;police&#39;: 5635,
 &#39;2police&#39;: 455,
 &#39;questioned&#39;: 5891,
 &#39;3wife&#39;: 525,
 &#39;siri&#39;: 6568,
 &#39;murder&#39;: 4959,
 &#39;4cook&#39;: 565,
 &#39;cooking&#39;: 2135,
 &#39;5gardener&#39;: 609,
 &#39;picking&#39;: 5539,
 &#39;vegetables&#39;: 7687,
 &#39;6housemaid&#39;: 660,
 &#39;went&#39;: 7888,
 &#39;post&#39;: 5677,
 &#39;7children&#39;: 692,
 &#39;play&#39;: 5581,
 &#39;8neighbour&#39;: 795,
 &#39;marriage&#39;: 4656,
 &#39;arrested&#39;: 1166,
 &#39;murderer&#39;: 4961,
 &#39;immediately&#39;: 3856,
 &#39;brilliant&#39;: 1604,
 &#39;petey&#39;: 5509,
 &#39;whereare&#39;: 7915,
 &#39;friendsare&#39;: 3192,
 &#39;thekingshead&#39;: 7240,
 &#39;canlove&#39;: 1755,
 &#39;nic&#39;: 5073,
 &#39;swimming&#39;: 7071,
 &#39;pool&#39;: 5652,
 &#39;jacuzzi&#39;: 4032,
 &#39;reached&#39;: 5958,
 &#39;sch&#39;: 6319,
 &#39;know&#39;: 4226,
 &#39;king&#39;: 4195,
 &#39;havin&#39;: 3563,
 &#39;goin2bed&#39;: 3363,
 &#39;only1more&#39;: 5286,
 &#39;sleep&#39;: 6608,
 &#39;inside&#39;: 3927,
 &#39;officestill&#39;: 5234,
 &#39;filling&#39;: 3017,
 &#39;formsdon&#39;: 3136,
 &#39;stress&#39;: 6928,
 &#39;dorm&#39;: 2582,
 &#39;details&#39;: 2436,
 &#39;money&#39;: 4874,
 &#39;mrng&#39;: 4920,
 &#39;panasonic&#39;: 5396,
 &#39;bluetoothhdset&#39;: 1503,
 &#39;motorola&#39;: 4906,
 &#39;doublemins&#39;: 2590,
 &#39;doubletxt&#39;: 2592,
 &#39;orange&#39;: 5318,
 &#39;contract&#39;: 2123,
 &#39;mobileupd8&#39;: 4852,
 &#39;08000839402&#39;: 43,
 &#39;2optout&#39;: 453,
 &#39;ilol&#39;: 3846,
 &#39;let&#39;: 4361,
 &#39;personally&#39;: 5501,
 &#39;wuldnt&#39;: 8078,
 &#39;bother&#39;: 1543,
 &#39;goin&#39;: 3362,
 &#39;mite&#39;: 4825,
 &#39;result&#39;: 6126,
 &#39;wheres&#39;: 7917,
 &#39;mummys&#39;: 4953,
 &#39;bad&#39;: 1288,
 &#39;positive&#39;: 5670,
 &#39;negative&#39;: 5037,
 &#39;mummy&#39;: 4952,
 &#39;made&#39;: 4595,
 &#39;hmmmm&#39;: 3671,
 &#39;kind&#39;: 4192,
 &#39;asthma&#39;: 1200,
 &#39;attack&#39;: 1213,
 &#39;nxt&#39;: 5199,
 &#39;hr&#39;: 3754,
 &#39;driving&#39;: 2625,
 &#39;park&#39;: 5422,
 &#39;people&#39;: 5476,
 &#39;wear&#39;: 7846,
 &#39;shirts&#39;: 6476,
 &#39;jumpers&#39;: 4120,
 &#39;hat&#39;: 3554,
 &#39;belt&#39;: 1409,
 &#39;cribbs&#39;: 2214,
 &#39;married&#39;: 4657,
 &#39;local&#39;: 4448,
 &#39;women&#39;: 8006,
 &#39;looking&#39;: 4480,
 &#39;discreet&#39;: 2515,
 &#39;action&#39;: 874,
 &#39;matches&#39;: 4669,
 &#39;instantly&#39;: 3933,
 &#39;69969&#39;: 655,
 &#39;cost&#39;: 2157,
 &#39;bcmsfwc1n3xx&#39;: 1358,
 &#39;imma&#39;: 3854,
 &#39;flip&#39;: 3075,
 &#39;ew&#39;: 2858,
 &#39;thought&#39;: 7285,
 &#39;slide&#39;: 6620,
 &#39;enough&#39;: 2779,
 &#39;avoid&#39;: 1253,
 &#39;unbelievable&#39;: 7565,
 &#39;faglord&#39;: 2922,
 &#39;laugh&#39;: 4306,
 &#39;loud&#39;: 4507,
 &#39;spontaneously&#39;: 6800,
 &#39;others&#39;: 5338,
 &#39;feel&#39;: 2981,
 &#39;best&#39;: 1418,
 &#39;goodevening&#39;: 3380,
 &#39;might&#39;: 4773,
 &#39;jays&#39;: 4051,
 &#39;sort&#39;: 6730,
 &#39;fucking&#39;: 3216,
 &#39;retard&#39;: 6130,
 &#39;pages&#39;: 5384,
 &#39;west&#39;: 7894,
 &#39;coast&#39;: 2004,
 &#39;haiz&#39;: 3508,
 &#39;ìïll&#39;: 8256,
 &#39;forever&#39;: 3120,
 &#39;celebrate&#39;: 1825,
 &#39;båõday&#39;: 1693,
 &#39;else&#39;: 2738,
 &#39;bot&#39;: 1542,
 &#39;notes&#39;: 5151,
 &#39;juz&#39;: 4126,
 &#39;rem&#39;: 6049,
 &#39;unless&#39;: 7594,
 &#39;gurl&#39;: 3482,
 &#39;would&#39;: 8052,
 &#39;appropriate&#39;: 1125,
 &#39;aww&#39;: 1264,
 &#39;possible&#39;: 5674,
 &#39;goodmorningmy&#39;: 3386,
 &#39;grandfather&#39;: 3423,
 &#39;expiredso&#39;: 2891,
 &#39;waaaat&#39;: 7771,
 &#39;lololo&#39;: 4468,
 &#39;aft&#39;: 929,
 &#39;850&#39;: 749,
 &#39;toa&#39;: 7354,
 &#39;payoh&#39;: 5458,
 &#39;650&#39;: 642,
 &#39;report&#39;: 6087,
 &#39;pay&#39;: 5452,
 &#39;salary&#39;: 6263,
 &#39;yes&#39;: 8176,
 &#39;princess&#39;: 5769,
 &#39;moan&#39;: 4845,
 &#39;charles&#39;: 1861,
 &#39;also&#39;: 1019,
 &#39;adding&#39;: 890,
 &#39;zeros&#39;: 8241,
 &#39;savings&#39;: 6304,
 &#39;checking&#39;: 1882,
 &#39;mandy&#39;: 4636,
 &#39;sullivan&#39;: 7003,
 &#39;calling&#39;: 1731,
 &#39;hotmix&#39;: 3734,
 &#39;fmyou&#39;: 3095,
 &#39;chosen&#39;: 1940,
 &#39;500000&#39;: 592,
 &#39;easter&#39;: 2684,
 &#39;drawplease&#39;: 2607,
 &#39;09041940223&#39;: 154,
 &#39;290305&#39;: 423,
 &#39;transferred&#39;: 7456,
 &#39;covers&#39;: 2185,
 &#39;face&#39;: 2913,
 &#39;kisses&#39;: 4201,
 &#39;hello&#39;: 3599,
 &#39;later&#39;: 4301,
 &#39;current&#39;: 2256,
 &#39;leading&#39;: 4325,
 &#39;bid&#39;: 1436,
 &#39;151&#39;: 335,
 &#39;pause&#39;: 5451,
 &#39;customer&#39;: 2264,
 &#39;08718726270&#39;: 134,
 ...}
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># How are these vectors stored?</span>
<span class="n">X_test_vect</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;1x8261 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;
	with 11 stored elements in Compressed Sparse Row format&gt;
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Can we convert the vectors to arrays?</span>
<span class="n">X_test_vect</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>array([[0., 0., 0., ..., 0., 0., 0.]])
</pre></div>
</div>
</section>
<section id="fit-random-forest-on-top-of-the-vectors">
<h4>Fit Random Forest on top of the Vectors<a class="headerlink" href="#fit-random-forest-on-top-of-the-vectors" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit a basic Random Forest model on these vectors</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_vect</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use the trained model to make predictions on the test data</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_vect</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate the predictions of the model on the holdout test set</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>

<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Precision: </span><span class="si">{}</span><span class="s1"> / Recall: </span><span class="si">{}</span><span class="s1"> / Accuracy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">round</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">((</span><span class="n">y_pred</span><span class="o">==</span><span class="n">y_test</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Precision: 1.0 / Recall: 0.844 / Accuracy: 0.978
</pre></div>
</div>
</section>
</section>
<section id="build-model-on-word2vec-vectors">
<h3>Build Model on word2vec Vectors<a class="headerlink" href="#build-model-on-word2vec-vectors" title="Permalink to this heading">#</a></h3>
<section id="create-word2vec-vectors">
<h4>Create word2vec Vectors:<a class="headerlink" href="#create-word2vec-vectors" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train a basic word2vec model</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span>
                                   <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                   <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                   <span class="n">min_count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Replace the words in each text message with the learned word vector</span>
<span class="n">words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">)</span>
<span class="n">X_train_vect</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ls</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
                         <span class="k">for</span> <span class="n">ls</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
<span class="n">X_test_vect</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ls</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
                         <span class="k">for</span> <span class="n">ls</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Average the word vectors for each sentence (and assign a vector of zeros if the model</span>
<span class="c1"># did not learn any of the words in the text message during training</span>
<span class="n">X_train_vect_avg</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">X_train_vect</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">X_train_vect_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X_train_vect_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">))</span>
        
<span class="n">X_test_vect_avg</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">X_test_vect</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">X_test_vect_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X_test_vect_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What does the unaveraged version look like?</span>
<span class="n">X_train_vect</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>array([[-6.6968979e-04,  3.4578454e-03, -1.9382305e-03, -4.7058719e-03,
         8.4614201e-04,  4.6616262e-03, -4.0404024e-03, -1.1704107e-03,
         3.0704867e-03,  1.5583877e-03,  3.3804949e-03,  4.1575865e-03,
        -1.5612100e-03,  2.3194156e-03, -7.7056163e-04, -3.4185851e-03,
         3.5160559e-04, -3.5576052e-03,  4.8053745e-04,  3.7461736e-03,
         6.0713897e-04,  4.0740389e-03,  3.7846118e-04, -1.5000689e-03,
        -1.0904194e-03,  2.2046040e-03, -5.0984434e-04,  1.0602624e-03,
        -3.0403486e-03,  1.5716453e-03, -3.1102828e-03, -2.1210681e-03,
         4.0272968e-03, -4.9504573e-03,  4.5761131e-03, -3.6794741e-03,
         1.3706160e-03,  2.3739457e-03, -2.3900673e-03, -4.9112695e-03,
        -1.2181470e-03,  3.6860681e-03,  1.0023477e-03,  3.6847258e-03,
        -1.4831536e-06, -1.4131200e-03,  1.6984355e-03, -3.0945004e-03,
         8.0331997e-04,  2.3724432e-03, -2.5922146e-03,  2.9519580e-03,
         4.0643511e-04,  1.0249075e-03, -4.5913369e-03, -4.7565927e-03,
        -6.9496373e-04,  1.4845592e-03, -5.0506077e-04,  4.1473093e-03,
        -3.8796372e-03,  4.8974189e-03, -2.6479717e-03,  4.2324234e-03,
        -2.8061483e-03, -4.0924819e-03,  8.6009532e-04, -1.6682795e-03,
         4.5245304e-03,  2.8416351e-03,  4.9983966e-03,  2.6125566e-03,
         4.6489555e-03,  2.0350993e-03,  3.6882376e-04, -2.2950210e-03,
         1.9338796e-03,  2.3064134e-04, -3.7733126e-03, -1.6300322e-03,
        -2.1535251e-03, -3.4281421e-03, -4.5618173e-03, -2.2069323e-03,
        -4.7594267e-03,  1.8748230e-03,  4.7056214e-03, -3.6993027e-03,
        -1.8026277e-03,  9.0052525e-04, -2.9262081e-03, -3.4789292e-03,
         4.9967961e-03,  2.2806867e-03, -1.0212690e-03,  4.6277726e-03,
        -2.4141204e-03, -3.6002311e-03, -4.2180084e-03,  4.4074645e-03],
       [-6.6968979e-04,  3.4578454e-03, -1.9382305e-03, -4.7058719e-03,
         8.4614201e-04,  4.6616262e-03, -4.0404024e-03, -1.1704107e-03,
         3.0704867e-03,  1.5583877e-03,  3.3804949e-03,  4.1575865e-03,
        -1.5612100e-03,  2.3194156e-03, -7.7056163e-04, -3.4185851e-03,
         3.5160559e-04, -3.5576052e-03,  4.8053745e-04,  3.7461736e-03,
         6.0713897e-04,  4.0740389e-03,  3.7846118e-04, -1.5000689e-03,
        -1.0904194e-03,  2.2046040e-03, -5.0984434e-04,  1.0602624e-03,
        -3.0403486e-03,  1.5716453e-03, -3.1102828e-03, -2.1210681e-03,
         4.0272968e-03, -4.9504573e-03,  4.5761131e-03, -3.6794741e-03,
         1.3706160e-03,  2.3739457e-03, -2.3900673e-03, -4.9112695e-03,
        -1.2181470e-03,  3.6860681e-03,  1.0023477e-03,  3.6847258e-03,
        -1.4831536e-06, -1.4131200e-03,  1.6984355e-03, -3.0945004e-03,
         8.0331997e-04,  2.3724432e-03, -2.5922146e-03,  2.9519580e-03,
         4.0643511e-04,  1.0249075e-03, -4.5913369e-03, -4.7565927e-03,
        -6.9496373e-04,  1.4845592e-03, -5.0506077e-04,  4.1473093e-03,
        -3.8796372e-03,  4.8974189e-03, -2.6479717e-03,  4.2324234e-03,
        -2.8061483e-03, -4.0924819e-03,  8.6009532e-04, -1.6682795e-03,
         4.5245304e-03,  2.8416351e-03,  4.9983966e-03,  2.6125566e-03,
         4.6489555e-03,  2.0350993e-03,  3.6882376e-04, -2.2950210e-03,
         1.9338796e-03,  2.3064134e-04, -3.7733126e-03, -1.6300322e-03,
        -2.1535251e-03, -3.4281421e-03, -4.5618173e-03, -2.2069323e-03,
        -4.7594267e-03,  1.8748230e-03,  4.7056214e-03, -3.6993027e-03,
        -1.8026277e-03,  9.0052525e-04, -2.9262081e-03, -3.4789292e-03,
         4.9967961e-03,  2.2806867e-03, -1.0212690e-03,  4.6277726e-03,
        -2.4141204e-03, -3.6002311e-03, -4.2180084e-03,  4.4074645e-03],
       [ 4.3485528e-03, -2.1064547e-03, -4.5112241e-03,  2.1121786e-03,
        -3.7960552e-03,  3.5110898e-03, -4.9450928e-03,  1.6422224e-03,
        -2.2383318e-03,  3.1312942e-03,  2.3281965e-03, -2.0045608e-03,
        -2.9566828e-03, -1.2722771e-03,  1.2139040e-03, -4.0501985e-03,
        -1.7297142e-03,  5.9885648e-04, -2.9098236e-03,  2.1682004e-03,
        -1.4990589e-03, -1.4968059e-03,  1.8113930e-03, -5.6124863e-04,
         3.5065075e-03, -3.9771781e-03, -1.3485128e-03, -1.5844227e-03,
        -1.5498535e-03,  5.9215654e-05, -4.6417961e-05, -2.9640200e-03,
         6.2372995e-04, -3.1896040e-03,  3.3416271e-03, -1.9149834e-03,
         2.3786009e-03, -2.1390673e-03,  4.1689556e-03, -2.0552076e-04,
         2.1500287e-03,  1.5448146e-03, -2.2303967e-03, -1.4130718e-03,
        -2.3505632e-03,  9.4632432e-04, -2.1969860e-03, -3.7429202e-03,
        -2.0193420e-03,  2.3205073e-03, -4.1863588e-03,  9.3078939e-05,
         1.3552760e-03,  2.8469104e-03,  3.2315899e-03, -3.1722602e-03,
        -1.0124577e-03,  1.1803993e-03, -4.3190131e-03, -1.3814573e-03,
        -4.9056429e-03,  8.2300576e-05,  1.7089667e-03, -3.9379919e-04,
        -2.7552552e-03, -3.1565584e-03,  3.0639779e-03,  5.5280351e-04,
         1.1857429e-03, -1.9806917e-03, -2.6994522e-03,  1.4180664e-04,
         3.6318211e-03,  2.9429712e-03,  9.4174687e-04, -3.1763765e-03,
        -3.7716888e-03, -4.2340718e-03, -4.3635732e-03, -3.3196269e-03,
         7.1189925e-04, -4.1868570e-03, -4.8751496e-03, -2.4618811e-03,
        -4.7011725e-03, -4.1731251e-03, -2.7461878e-03,  8.5527764e-04,
         1.8224501e-04,  3.3191708e-03,  4.8905966e-04, -2.3989929e-03,
        -1.0946748e-03,  5.0859293e-04, -1.0190058e-03, -2.9898202e-03,
        -4.0004717e-04,  3.2683087e-03, -3.4931551e-03,  4.9687489e-03],
       [ 4.3485528e-03, -2.1064547e-03, -4.5112241e-03,  2.1121786e-03,
        -3.7960552e-03,  3.5110898e-03, -4.9450928e-03,  1.6422224e-03,
        -2.2383318e-03,  3.1312942e-03,  2.3281965e-03, -2.0045608e-03,
        -2.9566828e-03, -1.2722771e-03,  1.2139040e-03, -4.0501985e-03,
        -1.7297142e-03,  5.9885648e-04, -2.9098236e-03,  2.1682004e-03,
        -1.4990589e-03, -1.4968059e-03,  1.8113930e-03, -5.6124863e-04,
         3.5065075e-03, -3.9771781e-03, -1.3485128e-03, -1.5844227e-03,
        -1.5498535e-03,  5.9215654e-05, -4.6417961e-05, -2.9640200e-03,
         6.2372995e-04, -3.1896040e-03,  3.3416271e-03, -1.9149834e-03,
         2.3786009e-03, -2.1390673e-03,  4.1689556e-03, -2.0552076e-04,
         2.1500287e-03,  1.5448146e-03, -2.2303967e-03, -1.4130718e-03,
        -2.3505632e-03,  9.4632432e-04, -2.1969860e-03, -3.7429202e-03,
        -2.0193420e-03,  2.3205073e-03, -4.1863588e-03,  9.3078939e-05,
         1.3552760e-03,  2.8469104e-03,  3.2315899e-03, -3.1722602e-03,
        -1.0124577e-03,  1.1803993e-03, -4.3190131e-03, -1.3814573e-03,
        -4.9056429e-03,  8.2300576e-05,  1.7089667e-03, -3.9379919e-04,
        -2.7552552e-03, -3.1565584e-03,  3.0639779e-03,  5.5280351e-04,
         1.1857429e-03, -1.9806917e-03, -2.6994522e-03,  1.4180664e-04,
         3.6318211e-03,  2.9429712e-03,  9.4174687e-04, -3.1763765e-03,
        -3.7716888e-03, -4.2340718e-03, -4.3635732e-03, -3.3196269e-03,
         7.1189925e-04, -4.1868570e-03, -4.8751496e-03, -2.4618811e-03,
        -4.7011725e-03, -4.1731251e-03, -2.7461878e-03,  8.5527764e-04,
         1.8224501e-04,  3.3191708e-03,  4.8905966e-04, -2.3989929e-03,
        -1.0946748e-03,  5.0859293e-04, -1.0190058e-03, -2.9898202e-03,
        -4.0004717e-04,  3.2683087e-03, -3.4931551e-03,  4.9687489e-03],
       [-6.6968979e-04,  3.4578454e-03, -1.9382305e-03, -4.7058719e-03,
         8.4614201e-04,  4.6616262e-03, -4.0404024e-03, -1.1704107e-03,
         3.0704867e-03,  1.5583877e-03,  3.3804949e-03,  4.1575865e-03,
        -1.5612100e-03,  2.3194156e-03, -7.7056163e-04, -3.4185851e-03,
         3.5160559e-04, -3.5576052e-03,  4.8053745e-04,  3.7461736e-03,
         6.0713897e-04,  4.0740389e-03,  3.7846118e-04, -1.5000689e-03,
        -1.0904194e-03,  2.2046040e-03, -5.0984434e-04,  1.0602624e-03,
        -3.0403486e-03,  1.5716453e-03, -3.1102828e-03, -2.1210681e-03,
         4.0272968e-03, -4.9504573e-03,  4.5761131e-03, -3.6794741e-03,
         1.3706160e-03,  2.3739457e-03, -2.3900673e-03, -4.9112695e-03,
        -1.2181470e-03,  3.6860681e-03,  1.0023477e-03,  3.6847258e-03,
        -1.4831536e-06, -1.4131200e-03,  1.6984355e-03, -3.0945004e-03,
         8.0331997e-04,  2.3724432e-03, -2.5922146e-03,  2.9519580e-03,
         4.0643511e-04,  1.0249075e-03, -4.5913369e-03, -4.7565927e-03,
        -6.9496373e-04,  1.4845592e-03, -5.0506077e-04,  4.1473093e-03,
        -3.8796372e-03,  4.8974189e-03, -2.6479717e-03,  4.2324234e-03,
        -2.8061483e-03, -4.0924819e-03,  8.6009532e-04, -1.6682795e-03,
         4.5245304e-03,  2.8416351e-03,  4.9983966e-03,  2.6125566e-03,
         4.6489555e-03,  2.0350993e-03,  3.6882376e-04, -2.2950210e-03,
         1.9338796e-03,  2.3064134e-04, -3.7733126e-03, -1.6300322e-03,
        -2.1535251e-03, -3.4281421e-03, -4.5618173e-03, -2.2069323e-03,
        -4.7594267e-03,  1.8748230e-03,  4.7056214e-03, -3.6993027e-03,
        -1.8026277e-03,  9.0052525e-04, -2.9262081e-03, -3.4789292e-03,
         4.9967961e-03,  2.2806867e-03, -1.0212690e-03,  4.6277726e-03,
        -2.4141204e-03, -3.6002311e-03, -4.2180084e-03,  4.4074645e-03]],
      dtype=float32)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What does the averaged version look like?</span>
<span class="n">X_train_vect_avg</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>array([ 1.3376072e-03,  1.2321254e-03, -2.9674280e-03, -1.9786519e-03,
       -1.0107368e-03,  4.2014113e-03, -4.4022785e-03, -4.5357436e-05,
        9.4695931e-04,  2.1875503e-03,  2.9595757e-03,  1.6927276e-03,
       -2.1193991e-03,  8.8273856e-04,  2.3224624e-05, -3.6712303e-03,
       -4.8092232e-04, -1.8950205e-03, -8.7560696e-04,  3.1149844e-03,
       -2.3534018e-04,  1.8457010e-03,  9.5163391e-04, -1.1245407e-03,
        7.4835139e-04, -2.6810885e-04, -8.4531168e-04,  2.3883999e-06,
       -2.4441506e-03,  9.6667337e-04, -1.8847368e-03, -2.4582488e-03,
        2.6658701e-03, -4.2461157e-03,  4.0823189e-03, -2.9736778e-03,
        1.7738100e-03,  5.6874048e-04,  2.3354185e-04, -3.0289697e-03,
        1.2912329e-04,  2.8295666e-03, -2.9075009e-04,  1.6456067e-03,
       -9.4111514e-04, -4.6934225e-04,  1.4026691e-04, -3.3538684e-03,
       -3.2574483e-04,  2.3516689e-03, -3.2298726e-03,  1.8084064e-03,
        7.8597141e-04,  1.7537087e-03, -1.4621662e-03, -4.1228598e-03,
       -8.2196126e-04,  1.3628952e-03, -2.0306418e-03,  1.9358027e-03,
       -4.2900397e-03,  2.9713716e-03, -9.0519636e-04,  2.3819343e-03,
       -2.7857912e-03, -3.7181124e-03,  1.7416483e-03, -7.7984633e-04,
        3.1890154e-03,  9.1270439e-04,  1.9192569e-03,  1.6242566e-03,
        4.2421017e-03,  2.3982481e-03,  5.9799303e-04, -2.6475631e-03,
       -3.4834776e-04, -1.5552441e-03, -4.0094168e-03, -2.3058702e-03,
       -1.0073554e-03, -3.7316282e-03, -4.6871500e-03, -2.3089119e-03,
       -4.7361250e-03, -5.4435618e-04,  1.7248977e-03, -1.8774705e-03,
       -1.0086786e-03,  1.8679835e-03, -1.5601009e-03, -3.0469545e-03,
        2.5602079e-03,  1.5718492e-03, -1.0203638e-03,  1.5807354e-03,
       -1.6084912e-03, -8.5281522e-04, -3.9280672e-03,  4.6319785e-03],
      dtype=float32)
</pre></div>
</div>
</section>
<section id="fit-randomforestclassifier-on-top-of-word2vec-vectors">
<h4>Fit RandomForestClassifier on top of word2vec Vectors:<a class="headerlink" href="#fit-randomforestclassifier-on-top-of-word2vec-vectors" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate and fit a basic Random Forest model on top of the vectors</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_vect_avg</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use the trained model to make predictions on the test data</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_vect_avg</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate the predictions of the model on the holdout test set</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>

<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Precision: </span><span class="si">{}</span><span class="s1"> / Recall: </span><span class="si">{}</span><span class="s1"> / Accuracy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">round</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">((</span><span class="n">y_pred</span><span class="o">==</span><span class="n">y_test</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Precision: 0.564 / Recall: 0.201 / Accuracy: 0.868
</pre></div>
</div>
</section>
</section>
<section id="build-model-on-doc2vec-vectors">
<h3>Build Model on doc2vec Vectors<a class="headerlink" href="#build-model-on-doc2vec-vectors" title="Permalink to this heading">#</a></h3>
<section id="creating-doc2vec-vectors">
<h4>Creating doc2vec Vectors:<a class="headerlink" href="#creating-doc2vec-vectors" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Created TaggedDocument vectors for each text message in the training and test sets</span>
<span class="n">tagged_docs_train</span> <span class="o">=</span> <span class="p">[</span><span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">doc2vec</span><span class="o">.</span><span class="n">TaggedDocument</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">])</span>
                     <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])]</span>
<span class="n">tagged_docs_test</span> <span class="o">=</span> <span class="p">[</span><span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">doc2vec</span><span class="o">.</span><span class="n">TaggedDocument</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># What do these TaggedDocument objects look like?</span>
<span class="n">tagged_docs_train</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[TaggedDocument(words=&quot;[&#39;check&#39;, &#39;rooms&#39;, &#39;befor&#39;, &#39;activities&#39;]&quot;, tags=[0]),
 TaggedDocument(words=&quot;[&#39;want&#39;, &#39;mapquest&#39;, &#39;something&#39;, &#39;look&#39;, &#39;usf&#39;, &#39;dogwood&#39;, &#39;drive&#39;]&quot;, tags=[1]),
 TaggedDocument(words=&quot;[&#39;ok&#39;, &#39;askd&#39;, &#39;u&#39;, &#39;knw&#39;, &#39;tht&#39;]&quot;, tags=[2]),
 TaggedDocument(words=&quot;[&#39;si&#39;, &#39;take&#39;, &#39;mokka&#39;, &#39;players&#39;]&quot;, tags=[3]),
 TaggedDocument(words=&quot;[&#39;dropped&#39;, &#39;em&#39;, &#39;omw&#39;, &#39;back&#39;]&quot;, tags=[4]),
 TaggedDocument(words=&quot;[&#39;come&#39;, &#39;tomorrow&#39;, &#39;di&#39;]&quot;, tags=[5]),
 TaggedDocument(words=&quot;[&#39;4&#39;, &#39;tacos&#39;, &#39;1&#39;, &#39;rajas&#39;, &#39;burrito&#39;, &#39;right&#39;]&quot;, tags=[6]),
 TaggedDocument(words=&quot;[&#39;oh&#39;, &#39;really&#39;, &#39;perform&#39;, &#39;write&#39;, &#39;paper&#39;, &#39;go&#39;, &#39;movie&#39;, &#39;home&#39;, &#39;midnight&#39;, &#39;huh&#39;]&quot;, tags=[7]),
 TaggedDocument(words=&quot;[&#39;u&#39;, &#39;dont&#39;, &#39;remember&#39;, &#39;old&#39;, &#39;commercial&#39;]&quot;, tags=[8]),
 TaggedDocument(words=&quot;[&#39;sms&#39;, &#39;services&#39;, &#39;inclusive&#39;, &#39;text&#39;, &#39;credits&#39;, &#39;pls&#39;, &#39;goto&#39;, &#39;wwwcomuknet&#39;, &#39;login&#39;, &#39;unsubscribe&#39;, &#39;stop&#39;, &#39;extra&#39;, &#39;charge&#39;, &#39;help08700469649&#39;, &#39;po&#39;, &#39;box420&#39;, &#39;ip4&#39;, &#39;5we&#39;]&quot;, tags=[9])]
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train a basic doc2vec model</span>
<span class="n">d2v_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Doc2Vec</span><span class="p">(</span><span class="n">tagged_docs_train</span><span class="p">,</span>
                                  <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                  <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                  <span class="n">min_count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Infer the vectors to be used in training and testing</span>
<span class="n">train_vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">infer_vector</span><span class="p">(</span><span class="nb">eval</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">words</span><span class="p">))</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tagged_docs_train</span><span class="p">]</span>
<span class="n">test_vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">infer_vector</span><span class="p">(</span><span class="nb">eval</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">words</span><span class="p">))</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tagged_docs_test</span><span class="p">]</span>   
</pre></div>
</div>
</section>
<section id="fit-randomforestclassifier-on-top-of-document-vectors">
<h4>Fit RandomForestClassifier on top of Document Vectors:<a class="headerlink" href="#fit-randomforestclassifier-on-top-of-document-vectors" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit a basic model, make predictions on the holdout test set, and the generate the evaluation metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_vectors</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_vectors</span><span class="p">)</span>

<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Precision: </span><span class="si">{}</span><span class="s1"> / Recall: </span><span class="si">{}</span><span class="s1"> / Accuracy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">round</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">((</span><span class="n">y_pred</span><span class="o">==</span><span class="n">y_test</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Precision: 0.753 / Recall: 0.357 / Accuracy: 0.895
</pre></div>
</div>
</section>
</section>
<section id="build-basic-rnn">
<h3>Build basic RNN<a class="headerlink" href="#build-basic-rnn" title="Permalink to this heading">#</a></h3>
<section id="prep-the-data-for-rnn">
<h4>Prep the data for RNN:<a class="headerlink" href="#prep-the-data-for-rnn" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the tokenizer and use that tokenizer to convert the sentences to sequences of numbers</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span>
<span class="n">X_train_seq</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span>
<span class="n">X_test_seq</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pad the sequences so each sequence is the same length</span>
<span class="n">X_train_seq_padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_train_seq</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">X_test_seq_padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_test_seq</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="build-and-evaluate-rnn">
<h4>Build and Evaluate RNN:<a class="headerlink" href="#build-and-evaluate-rnn" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the tools needed and use our previously defined functions to calculate precision and recall</span>
<span class="kn">import</span> <span class="nn">keras.backend</span> <span class="k">as</span> <span class="nn">K</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="k">def</span> <span class="nf">recall_m</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">true_positives</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">possible_positives</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">recall</span> <span class="o">=</span> <span class="n">true_positives</span> <span class="o">/</span> <span class="p">(</span><span class="n">possible_positives</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">epsilon</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">recall</span>

<span class="k">def</span> <span class="nf">precision_m</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">true_positives</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">predicted_positives</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="n">true_positives</span> <span class="o">/</span> <span class="p">(</span><span class="n">predicted_positives</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">epsilon</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">precision</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct our basic RNN model framework</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">index_word</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (None, None, 32)          265472    
_________________________________________________________________
lstm_2 (LSTM)                (None, 32)                8320      
_________________________________________________________________
dense_4 (Dense)              (None, 32)                1056      
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 33        
=================================================================
Total params: 274,881
Trainable params: 274,881
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compile the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">precision_m</span><span class="p">,</span> <span class="n">recall_m</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the RNN</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_seq_padded</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span> 
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test_seq_padded</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
140/140 [==============================] - 4s 18ms/step - loss: 0.3911 - accuracy: 0.8869 - precision_m: 0.1831 - recall_m: 0.1264 - val_loss: 0.0927 - val_accuracy: 0.9848 - val_precision_m: 0.9871 - val_recall_m: 0.9157
Epoch 2/10
140/140 [==============================] - 2s 14ms/step - loss: 0.0513 - accuracy: 0.9903 - precision_m: 0.9870 - recall_m: 0.9338 - val_loss: 0.0501 - val_accuracy: 0.9883 - val_precision_m: 1.0000 - val_recall_m: 0.9252
Epoch 3/10
140/140 [==============================] - 2s 14ms/step - loss: 0.0097 - accuracy: 0.9976 - precision_m: 0.9960 - recall_m: 0.9889 - val_loss: 0.0431 - val_accuracy: 0.9892 - val_precision_m: 0.9786 - val_recall_m: 0.9448
Epoch 4/10
140/140 [==============================] - 2s 15ms/step - loss: 0.0051 - accuracy: 0.9980 - precision_m: 0.9846 - recall_m: 0.9855 - val_loss: 0.0551 - val_accuracy: 0.9901 - val_precision_m: 0.9929 - val_recall_m: 0.9407
Epoch 5/10
140/140 [==============================] - 2s 16ms/step - loss: 5.4915e-04 - accuracy: 0.9999 - precision_m: 1.0000 - recall_m: 0.9993 - val_loss: 0.0603 - val_accuracy: 0.9910 - val_precision_m: 0.9929 - val_recall_m: 0.9448
Epoch 6/10
140/140 [==============================] - 2s 14ms/step - loss: 4.0873e-04 - accuracy: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9901 - val_precision_m: 0.9857 - val_recall_m: 0.9448
Epoch 7/10
140/140 [==============================] - 2s 14ms/step - loss: 1.5748e-04 - accuracy: 1.0000 - precision_m: 0.9876 - recall_m: 0.9876 - val_loss: 0.0697 - val_accuracy: 0.9910 - val_precision_m: 0.9929 - val_recall_m: 0.9448
Epoch 8/10
140/140 [==============================] - 2s 14ms/step - loss: 9.9006e-05 - accuracy: 1.0000 - precision_m: 0.9420 - recall_m: 0.9420 - val_loss: 0.0671 - val_accuracy: 0.9901 - val_precision_m: 0.9857 - val_recall_m: 0.9448
Epoch 9/10
140/140 [==============================] - 2s 15ms/step - loss: 1.0907e-04 - accuracy: 1.0000 - precision_m: 0.9815 - recall_m: 0.9815 - val_loss: 0.0727 - val_accuracy: 0.9901 - val_precision_m: 0.9857 - val_recall_m: 0.9448
Epoch 10/10
140/140 [==============================] - 2s 14ms/step - loss: 5.5320e-05 - accuracy: 1.0000 - precision_m: 0.9760 - recall_m: 0.9760 - val_loss: 0.0757 - val_accuracy: 0.9901 - val_precision_m: 0.9857 - val_recall_m: 0.9448
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot basic evaluation metrics across epochs</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;precision_m&#39;</span><span class="p">,</span> <span class="s1">&#39;recall_m&#39;</span><span class="p">]:</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Results for </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="_images/advanced_nlp_97_0.png" /></p>
<p><img alt="png" src="_images/advanced_nlp_97_1.png" /></p>
<p><img alt="png" src="_images/advanced_nlp_97_2.png" /></p>
</section>
</section>
</section>
<section id="summary-of-the-comparison">
<h2>Summary of the Comparison<a class="headerlink" href="#summary-of-the-comparison" title="Permalink to this heading">#</a></h2>
<section id="tf-idf">
<h3>TF-IDF<a class="headerlink" href="#tf-idf" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Simple document vectors that represent how important a word is to a document within a corpus</p></li>
<li><p>No consideration of context in which a word is used</p></li>
<li><p>Creates very sparse, large vectors</p></li>
</ul>
</section>
<section id="id1">
<h3>word2vec<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Word vectors created through a shallow, 2-layer neural network; word vectors can then be averaged or summed to create document level vectors</p></li>
<li><p>Creates smaller, dense vectors</p></li>
<li><p>Word vectors do have context built in</p></li>
</ul>
</section>
<section id="id2">
<h3>doc2vec<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Document vectors created through a shallow 2-layer neural network</p></li>
<li><p>Creates smaller, dense vectors</p></li>
<li><p>Document vectors do have context built in</p></li>
</ul>
</section>
<section id="id3">
<h3>Recurrent Neural Network (RNN)<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>A type of neural network that has an understanding of the data’s sequential nature (forms a sense of memory)</p></li>
<li><p>Dense vectors are created within the model</p></li>
</ul>
</section>
</section>
<section id="takeway">
<h2>Takeway<a class="headerlink" href="#takeway" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>TF-IDF is a quick and easy way to set a baseline</p></li>
<li><p>Informatoin is lost when averaging or summing word vectors</p></li>
<li><p>doc2vec is slower but better than word2vec for sentense vectors</p></li>
<li><p>RNN is extremely powerful even with limited data</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="nlp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Natural Language Processing (NLP)</p>
      </div>
    </a>
    <a class="right-next"
       href="deep_learning_apps.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Neural Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-pipeline">Basic pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#read-and-clean">Read and clean:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#randomforestclassifier">RandomForestClassifier</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">word2vec</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uses-cosine-similarity">uses “Cosine similarity”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explore-pre-trained-embeddings">Explore Pre-trained Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-our-own-model">Train our own model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prep-word-vectors">Prep word vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#doc2vec">doc2vec</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-our-own-model-for-doc2vec">Train our own model (for doc2vec)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-about-pre-trained-document-vectors">What About Pre-trained Document Vectors?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-based-vector-model">Deep Learning - based Vector model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-basic-rnn">Implement basic RNN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#read-clean-and-split-the-data">Read, clean and split the data:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prep-data-for-modelling">Prep data for modelling:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#build-model">Build model:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-the-nlp-techniques">Comparing the NLP Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-data-for-modelling">Preparing the data for modelling:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-model-on-tf-idf-vectors">Build Model on TF-IDF Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-tf-idf-vectors">Create TF-IDF Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-random-forest-on-top-of-the-vectors">Fit Random Forest on top of the Vectors</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-model-on-word2vec-vectors">Build Model on word2vec Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-word2vec-vectors">Create word2vec Vectors:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-randomforestclassifier-on-top-of-word2vec-vectors">Fit RandomForestClassifier on top of word2vec Vectors:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-model-on-doc2vec-vectors">Build Model on doc2vec Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-doc2vec-vectors">Creating doc2vec Vectors:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-randomforestclassifier-on-top-of-document-vectors">Fit RandomForestClassifier on top of Document Vectors:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-basic-rnn">Build basic RNN</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prep-the-data-for-rnn">Prep the data for RNN:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#build-and-evaluate-rnn">Build and Evaluate RNN:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-the-comparison">Summary of the Comparison</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf">TF-IDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">word2vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">doc2vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Recurrent Neural Network (RNN)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#takeway">Takeway</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lalit Prakash Vatsal
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>